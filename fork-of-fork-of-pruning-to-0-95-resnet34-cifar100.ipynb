{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T16:07:49.614085Z","iopub.status.busy":"2024-05-09T16:07:49.613632Z","iopub.status.idle":"2024-05-09T16:07:56.871272Z","shell.execute_reply":"2024-05-09T16:07:56.870258Z","shell.execute_reply.started":"2024-05-09T16:07:49.614050Z"},"trusted":true},"outputs":[],"source":["import os\n","import random\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","from torchvision import datasets, transforms\n","\n","import time\n","import copy\n","\n","import numpy as np\n","\n","import sklearn.metrics\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T16:08:05.575975Z","iopub.status.busy":"2024-05-09T16:08:05.575181Z","iopub.status.idle":"2024-05-09T16:08:05.581302Z","shell.execute_reply":"2024-05-09T16:08:05.580278Z","shell.execute_reply.started":"2024-05-09T16:08:05.575941Z"},"trusted":true},"outputs":[],"source":["def set_random_seeds(random_seed=0):\n","\n","    torch.manual_seed(random_seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    np.random.seed(random_seed)\n","    random.seed(random_seed)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T16:08:06.369040Z","iopub.status.busy":"2024-05-09T16:08:06.368669Z","iopub.status.idle":"2024-05-09T16:08:06.836751Z","shell.execute_reply":"2024-05-09T16:08:06.835825Z","shell.execute_reply.started":"2024-05-09T16:08:06.369010Z"},"trusted":true},"outputs":[],"source":["#import os\n","#import time\n","import math\n","#import random\n","#import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","import glob\n","\n","import matplotlib.pyplot as plt\n","from PIL import Image, ImageEnhance, ImageOps\n","\n","from tqdm import tqdm, tqdm_notebook\n","\n","import torch\n","from torch import nn, cuda\n","from torch.autograd import Variable \n","import torch.nn.functional as F\n","import torchvision as vision\n","import torchvision.models as models\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim import Adam, SGD, Optimizer\n","from torch.optim.lr_scheduler import _LRScheduler, CosineAnnealingLR, ReduceLROnPlateau\n","\n","from sklearn.metrics import f1_score\n","\n","class CIFAR10Policy(object):\n","    \"\"\" Randomly choose one of the best 25 Sub-policies on CIFAR10.\n","        Example:\n","        >>> policy = CIFAR10Policy()\n","        >>> transformed = policy(image)\n","        Example as a PyTorch Transform:\n","        >>> transform=transforms.Compose([\n","        >>>     transforms.Resize(256),\n","        >>>     CIFAR10Policy(),\n","        >>>     transforms.ToTensor()])\n","    \"\"\"\n","    def __init__(self, fillcolor=(128, 128, 128)):\n","        self.policies = [\n","            SubPolicy(0.1, \"invert\", 7, 0.2, \"contrast\", 6, fillcolor),\n","            SubPolicy(0.7, \"rotate\", 2, 0.3, \"translateX\", 9, fillcolor),\n","            SubPolicy(0.8, \"sharpness\", 1, 0.9, \"sharpness\", 3, fillcolor),\n","            SubPolicy(0.5, \"shearY\", 8, 0.7, \"translateY\", 9, fillcolor),\n","            SubPolicy(0.5, \"autocontrast\", 8, 0.9, \"equalize\", 2, fillcolor),\n","\n","            SubPolicy(0.2, \"shearY\", 7, 0.3, \"posterize\", 7, fillcolor),\n","            SubPolicy(0.4, \"color\", 3, 0.6, \"brightness\", 7, fillcolor),\n","            SubPolicy(0.3, \"sharpness\", 9, 0.7, \"brightness\", 9, fillcolor),\n","            SubPolicy(0.6, \"equalize\", 5, 0.5, \"equalize\", 1, fillcolor),\n","            SubPolicy(0.6, \"contrast\", 7, 0.6, \"sharpness\", 5, fillcolor),\n","\n","            SubPolicy(0.7, \"color\", 7, 0.5, \"translateX\", 8, fillcolor),\n","            SubPolicy(0.3, \"equalize\", 7, 0.4, \"autocontrast\", 8, fillcolor),\n","            SubPolicy(0.4, \"translateY\", 3, 0.2, \"sharpness\", 6, fillcolor),\n","            SubPolicy(0.9, \"brightness\", 6, 0.2, \"color\", 8, fillcolor),\n","            SubPolicy(0.5, \"solarize\", 2, 0.0, \"invert\", 3, fillcolor),\n","\n","            SubPolicy(0.2, \"equalize\", 0, 0.6, \"autocontrast\", 0, fillcolor),\n","            SubPolicy(0.2, \"equalize\", 8, 0.8, \"equalize\", 4, fillcolor),\n","            SubPolicy(0.9, \"color\", 9, 0.6, \"equalize\", 6, fillcolor),\n","            SubPolicy(0.8, \"autocontrast\", 4, 0.2, \"solarize\", 8, fillcolor),\n","            SubPolicy(0.1, \"brightness\", 3, 0.7, \"color\", 0, fillcolor),\n","\n","            SubPolicy(0.4, \"solarize\", 5, 0.9, \"autocontrast\", 3, fillcolor),\n","            SubPolicy(0.9, \"translateY\", 9, 0.7, \"translateY\", 9, fillcolor),\n","            SubPolicy(0.9, \"autocontrast\", 2, 0.8, \"solarize\", 3, fillcolor),\n","            SubPolicy(0.8, \"equalize\", 8, 0.1, \"invert\", 3, fillcolor),\n","            SubPolicy(0.7, \"translateY\", 9, 0.9, \"autocontrast\", 1, fillcolor)\n","        ]\n","\n","\n","    def __call__(self, img):\n","        policy_idx = random.randint(0, len(self.policies) - 1)\n","        return self.policies[policy_idx](img)\n","\n","    def __repr__(self):\n","        return \"AutoAugment CIFAR10 Policy\"\n","\n","\n","class SubPolicy(object):\n","    def __init__(self, p1, operation1, magnitude_idx1, p2, operation2, magnitude_idx2, fillcolor=(128, 128, 128)):\n","        ranges = {\n","            \"shearX\": np.linspace(0, 0.3, 10),\n","            \"shearY\": np.linspace(0, 0.3, 10),\n","            \"translateX\": np.linspace(0, 150 / 331, 10),\n","            \"translateY\": np.linspace(0, 150 / 331, 10),\n","            \"rotate\": np.linspace(0, 30, 10),\n","            \"color\": np.linspace(0.0, 0.9, 10),\n","            \"posterize\": np.round(np.linspace(8, 4, 10), 0).astype(int),\n","            \"solarize\": np.linspace(256, 0, 10),\n","            \"contrast\": np.linspace(0.0, 0.9, 10),\n","            \"sharpness\": np.linspace(0.0, 0.9, 10),\n","            \"brightness\": np.linspace(0.0, 0.9, 10),\n","            \"autocontrast\": [0] * 10,\n","            \"equalize\": [0] * 10,\n","            \"invert\": [0] * 10\n","        }\n","\n","        # from https://stackoverflow.com/questions/5252170/specify-image-filling-color-when-rotating-in-python-with-pil-and-setting-expand\n","        def rotate_with_fill(img, magnitude):\n","            rot = img.convert(\"RGBA\").rotate(magnitude)\n","            return Image.composite(rot, Image.new(\"RGBA\", rot.size, (128,) * 4), rot).convert(img.mode)\n","\n","        func = {\n","            \"shearX\": lambda img, magnitude: img.transform(\n","                img.size, Image.AFFINE, (1, magnitude * random.choice([-1, 1]), 0, 0, 1, 0),\n","                Image.BICUBIC, fillcolor=fillcolor),\n","            \"shearY\": lambda img, magnitude: img.transform(\n","                img.size, Image.AFFINE, (1, 0, 0, magnitude * random.choice([-1, 1]), 1, 0),\n","                Image.BICUBIC, fillcolor=fillcolor),\n","            \"translateX\": lambda img, magnitude: img.transform(\n","                img.size, Image.AFFINE, (1, 0, magnitude * img.size[0] * random.choice([-1, 1]), 0, 1, 0),\n","                fillcolor=fillcolor),\n","            \"translateY\": lambda img, magnitude: img.transform(\n","                img.size, Image.AFFINE, (1, 0, 0, 0, 1, magnitude * img.size[1] * random.choice([-1, 1])),\n","                fillcolor=fillcolor),\n","            \"rotate\": lambda img, magnitude: rotate_with_fill(img, magnitude),\n","            # \"rotate\": lambda img, magnitude: img.rotate(magnitude * random.choice([-1, 1])),\n","            \"color\": lambda img, magnitude: ImageEnhance.Color(img).enhance(1 + magnitude * random.choice([-1, 1])),\n","            \"posterize\": lambda img, magnitude: ImageOps.posterize(img, magnitude),\n","            \"solarize\": lambda img, magnitude: ImageOps.solarize(img, magnitude),\n","            \"contrast\": lambda img, magnitude: ImageEnhance.Contrast(img).enhance(\n","                1 + magnitude * random.choice([-1, 1])),\n","            \"sharpness\": lambda img, magnitude: ImageEnhance.Sharpness(img).enhance(\n","                1 + magnitude * random.choice([-1, 1])),\n","            \"brightness\": lambda img, magnitude: ImageEnhance.Brightness(img).enhance(\n","                1 + magnitude * random.choice([-1, 1])),\n","            \"autocontrast\": lambda img, magnitude: ImageOps.autocontrast(img),\n","            \"equalize\": lambda img, magnitude: ImageOps.equalize(img),\n","            \"invert\": lambda img, magnitude: ImageOps.invert(img)\n","        }\n","\n","        # self.name = \"{}_{:.2f}_and_{}_{:.2f}\".format(\n","        #     operation1, ranges[operation1][magnitude_idx1],\n","        #     operation2, ranges[operation2][magnitude_idx2])\n","        self.p1 = p1\n","        self.operation1 = func[operation1]\n","        self.magnitude1 = ranges[operation1][magnitude_idx1]\n","        self.p2 = p2\n","        self.operation2 = func[operation2]\n","        self.magnitude2 = ranges[operation2][magnitude_idx2]\n","\n","\n","    def __call__(self, img):\n","        if random.random() < self.p1: img = self.operation1(img, self.magnitude1)\n","        if random.random() < self.p2: img = self.operation2(img, self.magnitude2)\n","        return img\n","  \n","\n","class TestDataset(Dataset):\n","    def __init__(self, df, mode='test', transforms=None):\n","        self.df = df\n","        self.mode = mode\n","        self.transform = transforms[self.mode]\n","        \n","    def __len__(self):\n","        return len(self.df)\n","    \n","    def __getitem__(self, idx):\n","        \n","        image = Image.open(TEST_IMAGE_PATH / self.df[idx]).convert(\"RGB\")\n","        \n","        if self.transform:\n","            image = self.transform(image)\n","            \n","        return image"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T16:08:07.407391Z","iopub.status.busy":"2024-05-09T16:08:07.406580Z","iopub.status.idle":"2024-05-09T16:08:07.418203Z","shell.execute_reply":"2024-05-09T16:08:07.417242Z","shell.execute_reply.started":"2024-05-09T16:08:07.407358Z"},"trusted":true},"outputs":[],"source":["def prepare_dataloader(num_workers=0,\n","                       train_batch_size=128,\n","                       eval_batch_size=256,\n","                       mean=(0.4914, 0.4822, 0.4466),\n","                       stdev=(0.2412, 0.2377, 0.2563)):\n","\n","    train_transform = transforms.Compose([\n","        torchvision.transforms.Resize((224,224)),\n","        CIFAR10Policy(),\n","        torchvision.transforms.ToTensor(),\n","        torchvision.transforms.Normalize(mean=mean, std=stdev)\n","    ])\n","\n","    test_transform = transforms.Compose([\n","        torchvision.transforms.Resize((224,224)),\n","        torchvision.transforms.ToTensor(),\n","        torchvision.transforms.Normalize(mean=torch.tensor(mean), std=stdev)\n","    ])\n","\n","    train_set = torchvision.datasets.CIFAR100(root=\"data\",\n","                                             train=True,\n","                                             download=True,\n","                                             transform=train_transform)\n","\n","    test_set = torchvision.datasets.CIFAR100(root=\"data\",\n","                                            train=False,\n","                                            download=True,\n","                                            transform=test_transform)\n","\n","    train_sampler = torch.utils.data.RandomSampler(train_set)\n","    test_sampler = torch.utils.data.SequentialSampler(test_set)\n","\n","    train_loader = torch.utils.data.DataLoader(dataset=train_set,\n","                                               batch_size=train_batch_size,\n","                                               #shuffle=True,\n","                                               sampler=train_sampler,\n","                                               num_workers=num_workers,\n","                                               pin_memory=True\n","                                              )\n","\n","    test_loader = torch.utils.data.DataLoader(dataset=test_set,\n","                                              batch_size=eval_batch_size,\n","                                              #shuffle=False,\n","                                              sampler=test_sampler,\n","                                              num_workers=num_workers,\n","                                              pin_memory=True\n","                                             )\n","\n","    classes = train_set.classes\n","\n","    return train_loader, test_loader, classes"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T16:08:08.030176Z","iopub.status.busy":"2024-05-09T16:08:08.029898Z","iopub.status.idle":"2024-05-09T16:08:08.048219Z","shell.execute_reply":"2024-05-09T16:08:08.047335Z","shell.execute_reply.started":"2024-05-09T16:08:08.030152Z"},"trusted":true},"outputs":[],"source":["def train_model(model,\n","                train_loader,\n","                test_loader,\n","                device,\n","                #model_dir,\n","                #model_filename,\n","                l1_regularization_strength=0,\n","                l2_regularization_strength=0,\n","                weight_decay=5e-4,\n","                learning_rate=1e-4,\n","                num_epochs=200,\n","                checkpoint_epochs=20\n","                ):\n","\n","    start = time.time()\n","    criterion = nn.CrossEntropyLoss()\n","\n","    model.to(device)\n","\n","    #optimizer = optim.SGD(model.parameters(),\n","    #                      lr=learning_rate,\n","    #                      momentum=0.9,\n","    #                      weight_decay=l2_regularization_strength)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n","    # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=500)\n","    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n","                                                     milestones=[19, 28, 36],\n","                                                     gamma=0.1,\n","                                                     last_epoch=-1)\n","    # optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n","\n","    # Evaluation\n","    model.eval()\n","    eval_loss, eval_accuracy = evaluate_model(model=model,\n","                                              test_loader=test_loader,\n","                                              device=device,\n","                                              criterion=criterion)\n","    print(\"Epoch: {:03d} Eval Loss: {:.3f} Eval Acc: {:.3f}\".format(\n","        0, eval_loss, eval_accuracy))\n","\n","    for epoch in range(1, num_epochs+1):\n","        epoch_start = time.time()\n","        # Training\n","        model.train()\n","\n","        running_loss = 0\n","        running_corrects = 0\n","\n","        for inputs, labels in train_loader:\n","\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","\n","            # zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","            # forward + backward + optimize\n","            outputs = model(inputs)\n","            _, preds = torch.max(outputs, 1)\n","            loss = criterion(outputs, labels)\n","\n","            l1_reg = torch.tensor(0.).to(device)\n","            for module in model.modules():\n","                mask = None\n","                weight = None\n","                for name, buffer in module.named_buffers():\n","                    if name == \"weight_mask\":\n","                        mask = buffer\n","                for name, param in module.named_parameters():\n","                    if name == \"weight_orig\":\n","                        weight = param\n","                if mask is not None and weight is not None:\n","                    l1_reg += torch.norm(mask * weight, 1)\n","\n","            loss += l1_regularization_strength * l1_reg \n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            # statistics\n","            running_loss += loss.item() * inputs.size(0)\n","            running_corrects += torch.sum(preds == labels.data)\n","\n","        train_loss = running_loss / len(train_loader.dataset)\n","        train_accuracy = running_corrects / len(train_loader.dataset)\n","\n","        # Evaluation\n","        model.eval()\n","        eval_loss, eval_accuracy = evaluate_model(model=model,\n","                                                  test_loader=test_loader,\n","                                                  device=device,\n","                                                  criterion=criterion)\n","        if epoch % checkpoint_epochs == 0:\n","            torch.save({\n","                'epoch': epoch,\n","                'state_dict': model.state_dict(),\n","                'optimizer': optimizer.state_dict(),\n","            }, f'./checkpoint_epoch{epoch}.pth.tar')\n","\n","        scheduler.step()\n","        elapsed = time.time()\n","        print(\n","            \"Epoch: {:03d} Train Loss: {:.3f} Train Acc: {:.3f} Eval Loss: {:.3f} Eval Acc: {:.3f}\"\n","            .format(epoch, train_loss, train_accuracy, eval_loss,\n","                    eval_accuracy))\n","        print(f'Epoch time: {elapsed-epoch_start:.1f} Total training time: {elapsed-start:.1f}')\n","        print()\n","        #torch.cuda.empty_cache()\n","\n","    return model"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T16:08:09.150086Z","iopub.status.busy":"2024-05-09T16:08:09.149428Z","iopub.status.idle":"2024-05-09T16:08:09.158662Z","shell.execute_reply":"2024-05-09T16:08:09.157529Z","shell.execute_reply.started":"2024-05-09T16:08:09.150047Z"},"trusted":true},"outputs":[],"source":["def measure_module_sparsity(module, weight=True, bias=False, use_mask=False):\n","\n","    num_zeros = 0\n","    num_elements = 0\n","\n","    if use_mask == True:\n","        for buffer_name, buffer in module.named_buffers():\n","            if \"weight_mask\" in buffer_name and weight == True:\n","                num_zeros += torch.sum(buffer == 0).item()\n","                num_elements += buffer.nelement()\n","            if \"bias_mask\" in buffer_name and bias == True:\n","                num_zeros += torch.sum(buffer == 0).item()\n","                num_elements += buffer.nelement()\n","    else:\n","        for param_name, param in module.named_parameters():\n","            if \"weight\" in param_name and weight == True:\n","                num_zeros += torch.sum(param == 0).item()\n","                num_elements += param.nelement()\n","            if \"bias\" in param_name and bias == True:\n","                num_zeros += torch.sum(param == 0).item()\n","                num_elements += param.nelement()\n","\n","    sparsity = num_zeros / num_elements\n","\n","    return num_zeros, num_elements, sparsity"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T16:08:09.645866Z","iopub.status.busy":"2024-05-09T16:08:09.645256Z","iopub.status.idle":"2024-05-09T16:08:09.652862Z","shell.execute_reply":"2024-05-09T16:08:09.651767Z","shell.execute_reply.started":"2024-05-09T16:08:09.645833Z"},"trusted":true},"outputs":[],"source":["def measure_global_sparsity(model,\n","                            weight=True,\n","                            bias=False,\n","                            conv2d_use_mask=False,\n","                            linear_use_mask=False):\n","\n","    num_zeros = 0\n","    num_elements = 0\n","\n","    for module_name, module in model.named_modules():\n","\n","        if isinstance(module, torch.nn.Conv2d):\n","\n","            module_num_zeros, module_num_elements, _ = measure_module_sparsity(\n","                module, weight=weight, bias=bias, use_mask=conv2d_use_mask)\n","            num_zeros += module_num_zeros\n","            num_elements += module_num_elements\n","\n","        elif isinstance(module, torch.nn.Linear):\n","\n","            module_num_zeros, module_num_elements, _ = measure_module_sparsity(\n","                module, weight=weight, bias=bias, use_mask=linear_use_mask)\n","            num_zeros += module_num_zeros\n","            num_elements += module_num_elements\n","\n","    sparsity = num_zeros / num_elements\n","\n","    return num_zeros, num_elements, sparsity"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T16:08:10.125920Z","iopub.status.busy":"2024-05-09T16:08:10.125380Z","iopub.status.idle":"2024-05-09T16:08:10.132821Z","shell.execute_reply":"2024-05-09T16:08:10.131830Z","shell.execute_reply.started":"2024-05-09T16:08:10.125890Z"},"trusted":true},"outputs":[],"source":["def evaluate_model(model, test_loader, device, criterion=None):\n","\n","    model.eval()\n","    model.to(device)\n","\n","    running_loss = 0\n","    running_corrects = 0\n","\n","    for inputs, labels in test_loader:\n","\n","        inputs = inputs.to(device)\n","        labels = labels.to(device)\n","\n","        outputs = model(inputs)\n","        _, preds = torch.max(outputs, 1)\n","\n","        if criterion is not None:\n","            loss = criterion(outputs, labels).item()\n","        else:\n","            loss = 0\n","\n","        # statistics\n","        running_loss += loss * inputs.size(0)\n","        running_corrects += torch.sum(preds == labels.data)\n","        \n","        #torch.cuda.empty_cache()\n","\n","    eval_loss = running_loss / len(test_loader.dataset)\n","    eval_accuracy = running_corrects / len(test_loader.dataset)\n","    \n","\n","    return eval_loss, eval_accuracy"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T16:08:10.582838Z","iopub.status.busy":"2024-05-09T16:08:10.582215Z","iopub.status.idle":"2024-05-09T16:08:10.589224Z","shell.execute_reply":"2024-05-09T16:08:10.588286Z","shell.execute_reply.started":"2024-05-09T16:08:10.582802Z"},"trusted":true},"outputs":[],"source":["def create_classification_report(model, device, test_loader):\n","\n","    model.eval()\n","    model.to(device)\n","\n","    y_pred = []\n","    y_true = []\n","\n","    with torch.no_grad():\n","        for data in test_loader:\n","            y_true += data[1].numpy().tolist()\n","            images, _ = data[0].to(device), data[1].to(device)\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            y_pred += predicted.cpu().numpy().tolist()\n","\n","    classification_report = sklearn.metrics.classification_report(\n","        y_true=y_true, y_pred=y_pred)\n","\n","    return classification_report"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T16:08:11.053329Z","iopub.status.busy":"2024-05-09T16:08:11.052608Z","iopub.status.idle":"2024-05-09T16:08:11.058141Z","shell.execute_reply":"2024-05-09T16:08:11.057173Z","shell.execute_reply.started":"2024-05-09T16:08:11.053299Z"},"trusted":true},"outputs":[],"source":["def save_model(model, model_dir, model_filename):\n","\n","    if not os.path.exists(model_dir):\n","        os.makedirs(model_dir)\n","    model_filepath = os.path.join(model_dir, model_filename)\n","    torch.save(model.state_dict(), model_filepath)\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T16:08:11.487060Z","iopub.status.busy":"2024-05-09T16:08:11.486727Z","iopub.status.idle":"2024-05-09T16:08:11.491696Z","shell.execute_reply":"2024-05-09T16:08:11.490804Z","shell.execute_reply.started":"2024-05-09T16:08:11.487034Z"},"trusted":true},"outputs":[],"source":["def load_model(model, model_filepath, device):\n","\n","    model.load_state_dict(torch.load(model_filepath, map_location=device)['state_dict'])\n","\n","    return model"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T16:08:11.853701Z","iopub.status.busy":"2024-05-09T16:08:11.853354Z","iopub.status.idle":"2024-05-09T16:08:11.858870Z","shell.execute_reply":"2024-05-09T16:08:11.857951Z","shell.execute_reply.started":"2024-05-09T16:08:11.853673Z"},"trusted":true},"outputs":[],"source":["def create_model(num_classes=10, model_func=torchvision.models.resnet34):\n","\n","    model = model_func(num_classes=num_classes, pretrained=False)\n","\n","\n","    return model"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T16:08:12.282508Z","iopub.status.busy":"2024-05-09T16:08:12.282151Z","iopub.status.idle":"2024-05-09T16:08:12.299622Z","shell.execute_reply":"2024-05-09T16:08:12.298767Z","shell.execute_reply.started":"2024-05-09T16:08:12.282479Z"},"trusted":true},"outputs":[],"source":["def iterative_pruning_finetuning(model,\n","                                 train_loader,\n","                                 test_loader,\n","                                 device,\n","                                 learning_rate,\n","                                 l1_regularization_strength=0,\n","                                 l2_regularization_strength=0,\n","                                 weight_decay=5e-4,\n","                                 learning_rate_decay=0.6,\n","                                 conv2d_prune_amount=0.4,\n","                                 linear_prune_amount=0.2,\n","                                 num_iterations=10,\n","                                 num_epochs_per_iteration=10,\n","                                 #model_filename_prefix=\"pruned_model\",\n","                                 #model_dir=\"saved_models\",\n","                                 grouped_pruning=False):\n","\n","    conv2d_one_iter_prune_amount = 1 - (1 - conv2d_prune_amount)**(1/num_iterations)\n","    linear_one_iter_prune_amount = 1 - (1 - linear_prune_amount)**(1/num_iterations)\n","    for i in range(num_iterations):\n","\n","        print(\"Pruning and Finetuning {}/{}\".format(i + 1, num_iterations))\n","\n","        print(\"Pruning...\")\n","\n","        if grouped_pruning == True:\n","            \n","            parameters_to_prune = []\n","            for module_name, module in model.named_modules():\n","                if isinstance(module, torch.nn.Conv2d):\n","                    parameters_to_prune.append((module, \"weight\"))\n","            prune.global_unstructured(\n","                parameters_to_prune,\n","                pruning_method=prune.L1Unstructured,\n","                amount=conv2d_one_iter_prune_amount,\n","            )\n","        else:\n","            for module_name, module in model.named_modules():\n","                if isinstance(module, torch.nn.Conv2d):\n","                    prune.l1_unstructured(module,\n","                                          name=\"weight\",\n","                                          amount=conv2d_one_iter_prune_amount)\n","                elif isinstance(module, torch.nn.Linear):\n","                    prune.l1_unstructured(module,\n","                                          name=\"weight\",\n","                                          amount=linear_one_iter_prune_amount)\n","\n","        _, eval_accuracy = evaluate_model(model=model,\n","                                          test_loader=test_loader,\n","                                          device=device,\n","                                          criterion=None)\n","\n","        #classification_report = create_classification_report(\n","        #    model=model, test_loader=test_loader, device=device)\n","\n","        num_zeros, num_elements, sparsity = measure_global_sparsity(\n","            model,\n","            weight=True,\n","            bias=False,\n","            conv2d_use_mask=True,\n","            linear_use_mask=False)\n","\n","        print(\"Test Accuracy: {:.3f}\".format(eval_accuracy))\n","        #print(\"Classification Report:\")\n","        #print(classification_report)\n","        print(\"Global Sparsity:\")\n","        print(\"{:.4f}\".format(sparsity))\n","\n","        # print(model.conv1._forward_pre_hooks)\n","        \n","        if (i >= (num_iterations - 2)) and (num_iterations >= 3):\n","            cur_num_epochs_per_iter = int(num_epochs_per_iteration * 4/3)\n","        else:\n","            cur_num_epochs_per_iter = num_epochs_per_iteration\n","\n","        print(\"Fine-tuning...\")\n","\n","        train_model(model=model,\n","                    train_loader=train_loader,\n","                    test_loader=test_loader,\n","                    device=device,\n","                    #model_dir=model_dir,\n","                    #model_filename=\"{}_iter{}\".format(model_filename_prefix, i + 1),\n","                    l1_regularization_strength=l1_regularization_strength,\n","                    l2_regularization_strength=l2_regularization_strength,\n","                    weight_decay=weight_decay,\n","                    learning_rate=learning_rate * (learning_rate_decay**i),\n","                    num_epochs=cur_num_epochs_per_iter)\n","        \n","\n","        _, eval_accuracy = evaluate_model(model=model,\n","                                          test_loader=test_loader,\n","                                          device=device,\n","                                          criterion=None)\n","\n","        classification_report = create_classification_report(\n","            model=model, test_loader=test_loader, device=device)\n","\n","        num_zeros, num_elements, sparsity = measure_global_sparsity(\n","            model,\n","            weight=True,\n","            bias=False,\n","            conv2d_use_mask=True,\n","            linear_use_mask=False)\n","\n","        print(\"Test Accuracy: {:.3f}\".format(eval_accuracy))\n","        print(\"Classification Report:\")\n","        print(classification_report)\n","        print(\"Global Sparsity:\")\n","        print(\"{:.4f}\".format(sparsity))\n","\n","        #model_filename = \"{}_{}.pt\".format(model_filename_prefix, i + 1)\n","        #model_filepath = os.path.join(model_dir, model_filename)\n","        \n","        torch.save({\n","                #'epoch': epoch,\n","                'state_dict': model.state_dict(),\n","                #'optimizer': optimizer.state_dict(),\n","            }, f'./checkpoint_iter{i+1}.pth.tar')\n","        \n","        #save_model(model=model,\n","        #           model_dir=model_dir,\n","        #           model_filename=model_filename)\n","        \n","        #model = load_model(model=model,\n","        #                   model_filepath=model_filepath,\n","        #                   device=device)\n","        torch.cuda.empty_cache()\n","        \n","\n","    return model\n"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T16:08:12.694941Z","iopub.status.busy":"2024-05-09T16:08:12.694123Z","iopub.status.idle":"2024-05-09T16:08:12.701217Z","shell.execute_reply":"2024-05-09T16:08:12.700229Z","shell.execute_reply.started":"2024-05-09T16:08:12.694905Z"},"trusted":true},"outputs":[],"source":["def remove_parameters(model):\n","\n","    for module_name, module in model.named_modules():\n","        if isinstance(module, torch.nn.Conv2d):\n","            try:\n","                prune.remove(module, \"weight\")\n","            except:\n","                pass\n","            try:\n","                prune.remove(module, \"bias\")\n","            except:\n","                pass\n","        elif isinstance(module, torch.nn.Linear):\n","            try:\n","                prune.remove(module, \"weight\")\n","            except:\n","                pass\n","            try:\n","                prune.remove(module, \"bias\")\n","            except:\n","                pass\n","\n","    return model"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T16:08:13.222814Z","iopub.status.busy":"2024-05-09T16:08:13.222049Z","iopub.status.idle":"2024-05-09T16:08:13.228656Z","shell.execute_reply":"2024-05-09T16:08:13.227704Z","shell.execute_reply.started":"2024-05-09T16:08:13.222784Z"},"trusted":true},"outputs":[],"source":["import torch.nn.utils.prune as prune"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T16:08:14.454180Z","iopub.status.busy":"2024-05-09T16:08:14.453819Z","iopub.status.idle":"2024-05-09T16:08:14.459163Z","shell.execute_reply":"2024-05-09T16:08:14.457665Z","shell.execute_reply.started":"2024-05-09T16:08:14.454149Z"},"trusted":true},"outputs":[],"source":["model_filepath = \"/kaggle/input/resnet34-for-cifar100/pytorch/iteratively_pruned/2/sp0.95_acc0.814.pth.tar\""]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T16:08:15.774716Z","iopub.status.busy":"2024-05-09T16:08:15.774045Z","iopub.status.idle":"2024-05-09T16:08:15.778961Z","shell.execute_reply":"2024-05-09T16:08:15.778084Z","shell.execute_reply.started":"2024-05-09T16:08:15.774682Z"},"trusted":true},"outputs":[],"source":["num_classes = 100\n","random_seed = 2\n","l1_regularization_strength = 0\n","l2_regularization_strength = 0\n","weight_decay = 5e-4\n","learning_rate = 3e-5\n","learning_rate_decay = 1"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T16:08:16.126068Z","iopub.status.busy":"2024-05-09T16:08:16.125774Z","iopub.status.idle":"2024-05-09T16:08:16.139408Z","shell.execute_reply":"2024-05-09T16:08:16.138587Z","shell.execute_reply.started":"2024-05-09T16:08:16.126043Z"},"trusted":true},"outputs":[],"source":["mean = torch.tensor([0.5070, 0.4865, 0.4408]) # CIFAR100 train mean\n","std = torch.tensor([0.2621, 0.2512, 0.2713]) # CIFAR100 train std"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T16:08:16.985377Z","iopub.status.busy":"2024-05-09T16:08:16.984807Z","iopub.status.idle":"2024-05-09T16:08:17.012352Z","shell.execute_reply":"2024-05-09T16:08:17.011482Z","shell.execute_reply.started":"2024-05-09T16:08:16.985346Z"},"trusted":true},"outputs":[{"data":{"text/plain":["True"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["torch.cuda.is_available()"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T16:08:17.871103Z","iopub.status.busy":"2024-05-09T16:08:17.870747Z","iopub.status.idle":"2024-05-09T16:08:17.875938Z","shell.execute_reply":"2024-05-09T16:08:17.874709Z","shell.execute_reply.started":"2024-05-09T16:08:17.871076Z"},"trusted":true},"outputs":[],"source":["cuda_device = torch.device(\"cuda:0\")\n","cpu_device = torch.device(\"cpu:0\")"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T16:08:19.663008Z","iopub.status.busy":"2024-05-09T16:08:19.662357Z","iopub.status.idle":"2024-05-09T16:08:19.669544Z","shell.execute_reply":"2024-05-09T16:08:19.668808Z","shell.execute_reply.started":"2024-05-09T16:08:19.662977Z"},"trusted":true},"outputs":[],"source":["set_random_seeds(random_seed=random_seed)"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T16:08:20.608128Z","iopub.status.busy":"2024-05-09T16:08:20.607762Z","iopub.status.idle":"2024-05-09T16:08:25.373710Z","shell.execute_reply":"2024-05-09T16:08:25.372725Z","shell.execute_reply.started":"2024-05-09T16:08:20.608098Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n","  warnings.warn(msg)\n"]}],"source":["model = create_model(num_classes=num_classes)\n","parameters_to_prune = []\n","for module_name, module in model.named_modules():\n","    if isinstance(module, torch.nn.Conv2d):\n","        parameters_to_prune.append((module, \"weight\"))\n","prune.global_unstructured(\n","                parameters_to_prune,\n","                pruning_method=prune.L1Unstructured,\n","                amount=0,\n","            )\n","\n","    # Load a pretrained model.\n","model = load_model(model=model,\n","                    model_filepath=model_filepath,\n","                    device=cuda_device) # cuda_device!!!"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T16:08:29.408492Z","iopub.status.busy":"2024-05-09T16:08:29.408142Z","iopub.status.idle":"2024-05-09T16:08:36.563387Z","shell.execute_reply":"2024-05-09T16:08:36.562502Z","shell.execute_reply.started":"2024-05-09T16:08:29.408463Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_34/2226863683.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  torchvision.transforms.Normalize(mean=torch.tensor(mean), std=stdev)\n"]},{"name":"stdout","output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to data/cifar-100-python.tar.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 169001437/169001437 [00:03<00:00, 48631159.44it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting data/cifar-100-python.tar.gz to data\n","Files already downloaded and verified\n"]}],"source":["train_loader, test_loader, classes = prepare_dataloader(\n","        num_workers=2, train_batch_size=128, eval_batch_size=128, mean=mean, stdev=std)"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T16:08:38.067063Z","iopub.status.busy":"2024-05-09T16:08:38.066413Z","iopub.status.idle":"2024-05-09T16:08:49.381260Z","shell.execute_reply":"2024-05-09T16:08:49.380066Z","shell.execute_reply.started":"2024-05-09T16:08:38.067025Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(0.8137, device='cuda:0')\n"]}],"source":["\n","_, eval_accuracy = evaluate_model(model=model,\n","                                    test_loader=test_loader,\n","                                    device=cuda_device,\n","                                    criterion=None)\n","print(eval_accuracy)"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T16:08:56.507733Z","iopub.status.busy":"2024-05-09T16:08:56.507405Z","iopub.status.idle":"2024-05-09T16:08:56.518345Z","shell.execute_reply":"2024-05-09T16:08:56.517360Z","shell.execute_reply.started":"2024-05-09T16:08:56.507708Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Global Sparsity:\n","0.9491\n"]}],"source":["\n","num_zeros, num_elements, sparsity = measure_global_sparsity(model, conv2d_use_mask=True)\n","\n","#print(\"Test Accuracy: {:.3f}\".format(eval_accuracy))\n","#print(\"Classification Report:\")\n","#print(classification_report)\n","print(\"Global Sparsity:\")\n","print(\"{:.4f}\".format(sparsity))\n"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T16:09:00.099911Z","iopub.status.busy":"2024-05-09T16:09:00.099528Z","iopub.status.idle":"2024-05-09T16:09:01.104752Z","shell.execute_reply":"2024-05-09T16:09:01.103846Z","shell.execute_reply.started":"2024-05-09T16:09:00.099862Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Thu May  9 16:09:00 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla P100-PCIE-16GB           Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   39C    P0              33W / 250W |   8950MiB / 16384MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T16:09:16.630823Z","iopub.status.busy":"2024-05-09T16:09:16.630073Z","iopub.status.idle":"2024-05-09T16:09:16.635307Z","shell.execute_reply":"2024-05-09T16:09:16.634372Z","shell.execute_reply.started":"2024-05-09T16:09:16.630783Z"},"trusted":true},"outputs":[],"source":["import gc"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T16:09:26.100852Z","iopub.status.busy":"2024-05-09T16:09:26.100470Z","iopub.status.idle":"2024-05-09T16:09:26.561913Z","shell.execute_reply":"2024-05-09T16:09:26.560825Z","shell.execute_reply.started":"2024-05-09T16:09:26.100823Z"},"trusted":true},"outputs":[],"source":["gc.collect()\n","torch.cuda.empty_cache()\n","gc.collect()\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T16:09:27.873552Z","iopub.status.busy":"2024-05-09T16:09:27.873193Z","iopub.status.idle":"2024-05-09T16:09:28.877119Z","shell.execute_reply":"2024-05-09T16:09:28.876012Z","shell.execute_reply.started":"2024-05-09T16:09:27.873523Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Thu May  9 16:09:28 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla P100-PCIE-16GB           Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   39C    P0              37W / 250W |   2256MiB / 16384MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T16:09:31.234980Z","iopub.status.busy":"2024-05-09T16:09:31.234210Z","iopub.status.idle":"2024-05-09T16:09:31.241612Z","shell.execute_reply":"2024-05-09T16:09:31.240698Z","shell.execute_reply.started":"2024-05-09T16:09:31.234939Z"},"trusted":true},"outputs":[{"data":{"text/plain":["True"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["next(model.parameters()).is_cuda"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T16:09:37.621948Z","iopub.status.busy":"2024-05-09T16:09:37.621194Z","iopub.status.idle":"2024-05-09T19:24:09.502986Z","shell.execute_reply":"2024-05-09T19:24:09.501673Z","shell.execute_reply.started":"2024-05-09T16:09:37.621912Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Pruning...\n","Pruning and Finetuning 1/6\n","Pruning...\n","Test Accuracy: 0.075\n","Global Sparsity:\n","0.9646\n","Fine-tuning...\n","Epoch: 000 Eval Loss: 5.873 Eval Acc: 0.075\n","Epoch: 001 Train Loss: 1.325 Train Acc: 0.647 Eval Loss: 1.078 Eval Acc: 0.708\n","Epoch time: 143.0 Total training time: 154.1\n","\n","Epoch: 002 Train Loss: 0.799 Train Acc: 0.781 Eval Loss: 0.908 Eval Acc: 0.744\n","Epoch time: 142.8 Total training time: 296.9\n","\n","Epoch: 003 Train Loss: 0.633 Train Acc: 0.829 Eval Loss: 0.838 Eval Acc: 0.763\n","Epoch time: 142.6 Total training time: 439.5\n","\n","Epoch: 004 Train Loss: 0.547 Train Acc: 0.854 Eval Loss: 0.802 Eval Acc: 0.773\n","Epoch time: 142.9 Total training time: 582.4\n","\n","Epoch: 005 Train Loss: 0.484 Train Acc: 0.873 Eval Loss: 0.776 Eval Acc: 0.778\n","Epoch time: 142.3 Total training time: 724.7\n","\n","Epoch: 006 Train Loss: 0.434 Train Acc: 0.889 Eval Loss: 0.770 Eval Acc: 0.783\n","Epoch time: 142.6 Total training time: 867.3\n","\n","Epoch: 007 Train Loss: 0.411 Train Acc: 0.896 Eval Loss: 0.756 Eval Acc: 0.787\n","Epoch time: 142.7 Total training time: 1010.0\n","\n","Epoch: 008 Train Loss: 0.379 Train Acc: 0.905 Eval Loss: 0.748 Eval Acc: 0.789\n","Epoch time: 142.7 Total training time: 1152.7\n","\n","Epoch: 009 Train Loss: 0.356 Train Acc: 0.913 Eval Loss: 0.742 Eval Acc: 0.791\n","Epoch time: 142.6 Total training time: 1295.4\n","\n","Epoch: 010 Train Loss: 0.340 Train Acc: 0.916 Eval Loss: 0.739 Eval Acc: 0.792\n","Epoch time: 142.6 Total training time: 1437.9\n","\n","Epoch: 011 Train Loss: 0.333 Train Acc: 0.917 Eval Loss: 0.738 Eval Acc: 0.795\n","Epoch time: 142.8 Total training time: 1580.7\n","\n","Epoch: 012 Train Loss: 0.311 Train Acc: 0.925 Eval Loss: 0.735 Eval Acc: 0.792\n","Epoch time: 142.9 Total training time: 1723.6\n","\n","Test Accuracy: 0.792\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.90      0.94      0.92       100\n","           1       0.87      0.92      0.89       100\n","           2       0.64      0.72      0.68       100\n","           3       0.70      0.62      0.66       100\n","           4       0.60      0.69      0.64       100\n","           5       0.82      0.82      0.82       100\n","           6       0.80      0.80      0.80       100\n","           7       0.86      0.75      0.80       100\n","           8       0.94      0.90      0.92       100\n","           9       0.89      0.93      0.91       100\n","          10       0.79      0.61      0.69       100\n","          11       0.52      0.57      0.54       100\n","          12       0.83      0.90      0.86       100\n","          13       0.80      0.75      0.77       100\n","          14       0.88      0.85      0.86       100\n","          15       0.85      0.86      0.86       100\n","          16       0.82      0.83      0.83       100\n","          17       0.91      0.88      0.89       100\n","          18       0.75      0.76      0.76       100\n","          19       0.73      0.78      0.75       100\n","          20       0.89      0.91      0.90       100\n","          21       0.86      0.93      0.89       100\n","          22       0.81      0.84      0.82       100\n","          23       0.84      0.86      0.85       100\n","          24       0.88      0.89      0.89       100\n","          25       0.74      0.77      0.75       100\n","          26       0.78      0.73      0.76       100\n","          27       0.68      0.73      0.70       100\n","          28       0.86      0.82      0.84       100\n","          29       0.81      0.81      0.81       100\n","          30       0.72      0.77      0.74       100\n","          31       0.83      0.80      0.82       100\n","          32       0.78      0.73      0.75       100\n","          33       0.73      0.72      0.72       100\n","          34       0.85      0.80      0.82       100\n","          35       0.60      0.49      0.54       100\n","          36       0.80      0.85      0.83       100\n","          37       0.85      0.82      0.84       100\n","          38       0.74      0.79      0.76       100\n","          39       0.93      0.90      0.91       100\n","          40       0.88      0.80      0.84       100\n","          41       0.88      0.90      0.89       100\n","          42       0.80      0.81      0.81       100\n","          43       0.91      0.87      0.89       100\n","          44       0.74      0.60      0.66       100\n","          45       0.67      0.73      0.70       100\n","          46       0.61      0.67      0.64       100\n","          47       0.59      0.64      0.61       100\n","          48       0.87      0.97      0.92       100\n","          49       0.85      0.89      0.87       100\n","          50       0.65      0.68      0.67       100\n","          51       0.78      0.82      0.80       100\n","          52       0.61      0.62      0.62       100\n","          53       0.88      0.96      0.92       100\n","          54       0.86      0.83      0.84       100\n","          55       0.54      0.54      0.54       100\n","          56       0.91      0.93      0.92       100\n","          57       0.88      0.87      0.87       100\n","          58       0.90      0.94      0.92       100\n","          59       0.80      0.69      0.74       100\n","          60       0.84      0.81      0.82       100\n","          61       0.77      0.78      0.78       100\n","          62       0.81      0.78      0.80       100\n","          63       0.76      0.76      0.76       100\n","          64       0.69      0.61      0.65       100\n","          65       0.74      0.70      0.72       100\n","          66       0.92      0.84      0.88       100\n","          67       0.80      0.68      0.74       100\n","          68       0.90      0.97      0.93       100\n","          69       0.79      0.85      0.82       100\n","          70       0.79      0.77      0.78       100\n","          71       0.76      0.81      0.78       100\n","          72       0.53      0.55      0.54       100\n","          73       0.71      0.72      0.72       100\n","          74       0.64      0.63      0.64       100\n","          75       0.91      0.90      0.90       100\n","          76       0.93      0.89      0.91       100\n","          77       0.83      0.79      0.81       100\n","          78       0.76      0.66      0.71       100\n","          79       0.79      0.85      0.82       100\n","          80       0.68      0.80      0.73       100\n","          81       0.72      0.81      0.76       100\n","          82       0.94      0.93      0.93       100\n","          83       0.88      0.81      0.84       100\n","          84       0.88      0.81      0.84       100\n","          85       0.88      0.92      0.90       100\n","          86       0.87      0.82      0.85       100\n","          87       0.89      0.90      0.90       100\n","          88       0.87      0.77      0.81       100\n","          89       0.91      0.86      0.89       100\n","          90       0.79      0.81      0.80       100\n","          91       0.89      0.90      0.90       100\n","          92       0.70      0.76      0.73       100\n","          93       0.83      0.74      0.78       100\n","          94       0.90      0.96      0.93       100\n","          95       0.72      0.76      0.74       100\n","          96       0.67      0.71      0.69       100\n","          97       0.81      0.81      0.81       100\n","          98       0.65      0.58      0.61       100\n","          99       0.74      0.84      0.79       100\n","\n","    accuracy                           0.79     10000\n","   macro avg       0.79      0.79      0.79     10000\n","weighted avg       0.79      0.79      0.79     10000\n","\n","Global Sparsity:\n","0.9646\n","Pruning and Finetuning 2/6\n","Pruning...\n","Test Accuracy: 0.098\n","Global Sparsity:\n","0.9751\n","Fine-tuning...\n","Epoch: 000 Eval Loss: 5.348 Eval Acc: 0.098\n","Epoch: 001 Train Loss: 1.815 Train Acc: 0.526 Eval Loss: 1.321 Eval Acc: 0.632\n","Epoch time: 142.7 Total training time: 153.5\n","\n","Epoch: 002 Train Loss: 1.163 Train Acc: 0.683 Eval Loss: 1.073 Eval Acc: 0.690\n","Epoch time: 142.8 Total training time: 296.3\n","\n","Epoch: 003 Train Loss: 0.950 Train Acc: 0.739 Eval Loss: 0.962 Eval Acc: 0.720\n","Epoch time: 142.8 Total training time: 439.0\n","\n","Epoch: 004 Train Loss: 0.826 Train Acc: 0.774 Eval Loss: 0.898 Eval Acc: 0.740\n","Epoch time: 142.7 Total training time: 581.8\n","\n","Epoch: 005 Train Loss: 0.746 Train Acc: 0.797 Eval Loss: 0.856 Eval Acc: 0.753\n","Epoch time: 142.8 Total training time: 724.6\n","\n","Epoch: 006 Train Loss: 0.680 Train Acc: 0.816 Eval Loss: 0.834 Eval Acc: 0.762\n","Epoch time: 142.6 Total training time: 867.1\n","\n","Epoch: 007 Train Loss: 0.640 Train Acc: 0.829 Eval Loss: 0.818 Eval Acc: 0.764\n","Epoch time: 142.9 Total training time: 1010.1\n","\n","Epoch: 008 Train Loss: 0.597 Train Acc: 0.840 Eval Loss: 0.798 Eval Acc: 0.769\n","Epoch time: 142.5 Total training time: 1152.6\n","\n","Epoch: 009 Train Loss: 0.570 Train Acc: 0.846 Eval Loss: 0.781 Eval Acc: 0.774\n","Epoch time: 142.7 Total training time: 1295.3\n","\n","Epoch: 010 Train Loss: 0.544 Train Acc: 0.855 Eval Loss: 0.778 Eval Acc: 0.777\n","Epoch time: 142.5 Total training time: 1437.8\n","\n","Epoch: 011 Train Loss: 0.516 Train Acc: 0.866 Eval Loss: 0.771 Eval Acc: 0.779\n","Epoch time: 142.6 Total training time: 1580.4\n","\n","Epoch: 012 Train Loss: 0.492 Train Acc: 0.871 Eval Loss: 0.771 Eval Acc: 0.775\n","Epoch time: 142.7 Total training time: 1723.1\n","\n","Test Accuracy: 0.775\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.94      0.91      0.92       100\n","           1       0.91      0.88      0.89       100\n","           2       0.65      0.64      0.65       100\n","           3       0.57      0.63      0.60       100\n","           4       0.59      0.70      0.64       100\n","           5       0.76      0.72      0.74       100\n","           6       0.80      0.79      0.79       100\n","           7       0.79      0.77      0.78       100\n","           8       0.94      0.94      0.94       100\n","           9       0.88      0.91      0.89       100\n","          10       0.74      0.60      0.66       100\n","          11       0.52      0.54      0.53       100\n","          12       0.83      0.85      0.84       100\n","          13       0.78      0.63      0.70       100\n","          14       0.83      0.77      0.80       100\n","          15       0.76      0.81      0.79       100\n","          16       0.80      0.85      0.83       100\n","          17       0.93      0.87      0.90       100\n","          18       0.74      0.81      0.78       100\n","          19       0.79      0.72      0.75       100\n","          20       0.87      0.87      0.87       100\n","          21       0.88      0.92      0.90       100\n","          22       0.79      0.81      0.80       100\n","          23       0.87      0.84      0.85       100\n","          24       0.84      0.81      0.82       100\n","          25       0.72      0.73      0.73       100\n","          26       0.77      0.72      0.75       100\n","          27       0.69      0.71      0.70       100\n","          28       0.88      0.82      0.85       100\n","          29       0.80      0.84      0.82       100\n","          30       0.74      0.78      0.76       100\n","          31       0.73      0.78      0.75       100\n","          32       0.74      0.77      0.75       100\n","          33       0.75      0.66      0.70       100\n","          34       0.79      0.81      0.80       100\n","          35       0.68      0.46      0.55       100\n","          36       0.81      0.88      0.84       100\n","          37       0.79      0.84      0.82       100\n","          38       0.68      0.67      0.67       100\n","          39       0.93      0.92      0.92       100\n","          40       0.79      0.74      0.76       100\n","          41       0.89      0.88      0.88       100\n","          42       0.77      0.84      0.80       100\n","          43       0.83      0.84      0.84       100\n","          44       0.72      0.68      0.70       100\n","          45       0.65      0.74      0.69       100\n","          46       0.55      0.57      0.56       100\n","          47       0.64      0.64      0.64       100\n","          48       0.91      0.96      0.94       100\n","          49       0.81      0.83      0.82       100\n","          50       0.61      0.63      0.62       100\n","          51       0.80      0.81      0.81       100\n","          52       0.59      0.69      0.64       100\n","          53       0.85      0.95      0.90       100\n","          54       0.81      0.84      0.82       100\n","          55       0.53      0.56      0.54       100\n","          56       0.90      0.95      0.93       100\n","          57       0.84      0.85      0.85       100\n","          58       0.90      0.94      0.92       100\n","          59       0.78      0.69      0.73       100\n","          60       0.86      0.82      0.84       100\n","          61       0.72      0.79      0.76       100\n","          62       0.77      0.76      0.76       100\n","          63       0.73      0.73      0.73       100\n","          64       0.69      0.62      0.65       100\n","          65       0.72      0.71      0.71       100\n","          66       0.93      0.81      0.87       100\n","          67       0.75      0.66      0.70       100\n","          68       0.92      0.90      0.91       100\n","          69       0.80      0.87      0.83       100\n","          70       0.79      0.73      0.76       100\n","          71       0.74      0.79      0.76       100\n","          72       0.53      0.48      0.51       100\n","          73       0.72      0.71      0.72       100\n","          74       0.65      0.57      0.61       100\n","          75       0.93      0.86      0.90       100\n","          76       0.87      0.88      0.88       100\n","          77       0.86      0.74      0.80       100\n","          78       0.73      0.72      0.72       100\n","          79       0.79      0.84      0.82       100\n","          80       0.68      0.72      0.70       100\n","          81       0.70      0.80      0.75       100\n","          82       0.96      0.93      0.94       100\n","          83       0.79      0.77      0.78       100\n","          84       0.73      0.72      0.73       100\n","          85       0.87      0.89      0.88       100\n","          86       0.81      0.80      0.80       100\n","          87       0.85      0.90      0.87       100\n","          88       0.84      0.79      0.81       100\n","          89       0.85      0.94      0.90       100\n","          90       0.83      0.79      0.81       100\n","          91       0.88      0.85      0.86       100\n","          92       0.62      0.68      0.65       100\n","          93       0.78      0.73      0.76       100\n","          94       0.89      0.93      0.91       100\n","          95       0.78      0.73      0.76       100\n","          96       0.61      0.63      0.62       100\n","          97       0.84      0.86      0.85       100\n","          98       0.58      0.67      0.62       100\n","          99       0.81      0.81      0.81       100\n","\n","    accuracy                           0.78     10000\n","   macro avg       0.78      0.78      0.78     10000\n","weighted avg       0.78      0.78      0.78     10000\n","\n","Global Sparsity:\n","0.9751\n","Pruning and Finetuning 3/6\n","Pruning...\n","Test Accuracy: 0.034\n","Global Sparsity:\n","0.9823\n","Fine-tuning...\n","Epoch: 000 Eval Loss: 9.776 Eval Acc: 0.034\n","Epoch: 001 Train Loss: 1.944 Train Acc: 0.489 Eval Loss: 1.431 Eval Acc: 0.604\n","Epoch time: 142.8 Total training time: 153.8\n","\n","Epoch: 002 Train Loss: 1.391 Train Acc: 0.622 Eval Loss: 1.193 Eval Acc: 0.663\n","Epoch time: 142.9 Total training time: 296.6\n","\n","Epoch: 003 Train Loss: 1.180 Train Acc: 0.681 Eval Loss: 1.073 Eval Acc: 0.696\n","Epoch time: 142.6 Total training time: 439.2\n","\n","Epoch: 004 Train Loss: 1.054 Train Acc: 0.712 Eval Loss: 1.004 Eval Acc: 0.711\n","Epoch time: 142.8 Total training time: 582.0\n","\n","Epoch: 005 Train Loss: 0.968 Train Acc: 0.734 Eval Loss: 0.961 Eval Acc: 0.720\n","Epoch time: 142.9 Total training time: 724.9\n","\n","Epoch: 006 Train Loss: 0.911 Train Acc: 0.749 Eval Loss: 0.925 Eval Acc: 0.731\n","Epoch time: 142.7 Total training time: 867.6\n","\n","Epoch: 007 Train Loss: 0.858 Train Acc: 0.765 Eval Loss: 0.900 Eval Acc: 0.739\n","Epoch time: 142.5 Total training time: 1010.1\n","\n","Epoch: 008 Train Loss: 0.816 Train Acc: 0.776 Eval Loss: 0.882 Eval Acc: 0.742\n","Epoch time: 142.9 Total training time: 1153.0\n","\n","Epoch: 009 Train Loss: 0.779 Train Acc: 0.788 Eval Loss: 0.870 Eval Acc: 0.744\n","Epoch time: 142.6 Total training time: 1295.6\n","\n","Epoch: 010 Train Loss: 0.754 Train Acc: 0.793 Eval Loss: 0.861 Eval Acc: 0.749\n","Epoch time: 142.8 Total training time: 1438.4\n","\n","Epoch: 011 Train Loss: 0.728 Train Acc: 0.802 Eval Loss: 0.850 Eval Acc: 0.750\n","Epoch time: 142.6 Total training time: 1581.0\n","\n","Epoch: 012 Train Loss: 0.704 Train Acc: 0.809 Eval Loss: 0.840 Eval Acc: 0.752\n","Epoch time: 143.0 Total training time: 1724.0\n","\n","Test Accuracy: 0.752\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.92      0.87      0.89       100\n","           1       0.87      0.88      0.88       100\n","           2       0.61      0.67      0.64       100\n","           3       0.61      0.53      0.57       100\n","           4       0.60      0.64      0.62       100\n","           5       0.70      0.71      0.71       100\n","           6       0.78      0.80      0.79       100\n","           7       0.75      0.73      0.74       100\n","           8       0.94      0.89      0.91       100\n","           9       0.91      0.90      0.90       100\n","          10       0.69      0.55      0.61       100\n","          11       0.54      0.49      0.52       100\n","          12       0.73      0.79      0.76       100\n","          13       0.81      0.62      0.70       100\n","          14       0.80      0.78      0.79       100\n","          15       0.66      0.79      0.72       100\n","          16       0.82      0.78      0.80       100\n","          17       0.85      0.86      0.86       100\n","          18       0.73      0.77      0.75       100\n","          19       0.75      0.70      0.73       100\n","          20       0.85      0.88      0.87       100\n","          21       0.76      0.87      0.81       100\n","          22       0.78      0.76      0.77       100\n","          23       0.88      0.80      0.84       100\n","          24       0.88      0.87      0.87       100\n","          25       0.69      0.69      0.69       100\n","          26       0.77      0.75      0.76       100\n","          27       0.67      0.70      0.68       100\n","          28       0.82      0.80      0.81       100\n","          29       0.77      0.79      0.78       100\n","          30       0.77      0.73      0.75       100\n","          31       0.70      0.68      0.69       100\n","          32       0.70      0.73      0.72       100\n","          33       0.74      0.70      0.72       100\n","          34       0.79      0.74      0.76       100\n","          35       0.54      0.55      0.55       100\n","          36       0.79      0.85      0.82       100\n","          37       0.83      0.82      0.82       100\n","          38       0.64      0.70      0.67       100\n","          39       0.91      0.86      0.88       100\n","          40       0.74      0.69      0.72       100\n","          41       0.87      0.90      0.88       100\n","          42       0.82      0.74      0.78       100\n","          43       0.73      0.81      0.77       100\n","          44       0.63      0.62      0.62       100\n","          45       0.63      0.67      0.65       100\n","          46       0.60      0.52      0.56       100\n","          47       0.62      0.64      0.63       100\n","          48       0.89      0.93      0.91       100\n","          49       0.82      0.89      0.86       100\n","          50       0.61      0.58      0.59       100\n","          51       0.79      0.79      0.79       100\n","          52       0.60      0.66      0.63       100\n","          53       0.78      0.94      0.85       100\n","          54       0.84      0.86      0.85       100\n","          55       0.49      0.43      0.46       100\n","          56       0.83      0.90      0.87       100\n","          57       0.82      0.80      0.81       100\n","          58       0.87      0.88      0.88       100\n","          59       0.70      0.58      0.63       100\n","          60       0.85      0.81      0.83       100\n","          61       0.74      0.75      0.75       100\n","          62       0.71      0.77      0.74       100\n","          63       0.67      0.71      0.69       100\n","          64       0.71      0.60      0.65       100\n","          65       0.64      0.62      0.63       100\n","          66       0.81      0.78      0.80       100\n","          67       0.70      0.61      0.65       100\n","          68       0.85      0.90      0.87       100\n","          69       0.81      0.87      0.84       100\n","          70       0.75      0.73      0.74       100\n","          71       0.72      0.78      0.75       100\n","          72       0.51      0.41      0.46       100\n","          73       0.66      0.67      0.67       100\n","          74       0.59      0.58      0.58       100\n","          75       0.92      0.87      0.89       100\n","          76       0.90      0.88      0.89       100\n","          77       0.76      0.76      0.76       100\n","          78       0.64      0.64      0.64       100\n","          79       0.77      0.82      0.79       100\n","          80       0.61      0.68      0.64       100\n","          81       0.74      0.78      0.76       100\n","          82       0.93      0.90      0.91       100\n","          83       0.83      0.71      0.76       100\n","          84       0.73      0.69      0.71       100\n","          85       0.84      0.86      0.85       100\n","          86       0.77      0.77      0.77       100\n","          87       0.88      0.89      0.89       100\n","          88       0.84      0.82      0.83       100\n","          89       0.86      0.87      0.87       100\n","          90       0.74      0.81      0.78       100\n","          91       0.84      0.87      0.86       100\n","          92       0.64      0.65      0.64       100\n","          93       0.70      0.71      0.70       100\n","          94       0.88      0.96      0.92       100\n","          95       0.79      0.75      0.77       100\n","          96       0.65      0.73      0.69       100\n","          97       0.79      0.83      0.81       100\n","          98       0.56      0.63      0.59       100\n","          99       0.81      0.83      0.82       100\n","\n","    accuracy                           0.75     10000\n","   macro avg       0.75      0.75      0.75     10000\n","weighted avg       0.75      0.75      0.75     10000\n","\n","Global Sparsity:\n","0.9823\n","Pruning and Finetuning 4/6\n","Pruning...\n","Test Accuracy: 0.072\n","Global Sparsity:\n","0.9872\n","Fine-tuning...\n","Epoch: 000 Eval Loss: 6.561 Eval Acc: 0.072\n","Epoch: 001 Train Loss: 2.218 Train Acc: 0.427 Eval Loss: 1.562 Eval Acc: 0.577\n","Epoch time: 142.8 Total training time: 153.9\n","\n","Epoch: 002 Train Loss: 1.603 Train Acc: 0.571 Eval Loss: 1.306 Eval Acc: 0.634\n","Epoch time: 142.5 Total training time: 296.4\n","\n","Epoch: 003 Train Loss: 1.390 Train Acc: 0.625 Eval Loss: 1.178 Eval Acc: 0.663\n","Epoch time: 142.7 Total training time: 439.1\n","\n","Epoch: 004 Train Loss: 1.268 Train Acc: 0.656 Eval Loss: 1.112 Eval Acc: 0.674\n","Epoch time: 142.5 Total training time: 581.6\n","\n","Epoch: 005 Train Loss: 1.178 Train Acc: 0.680 Eval Loss: 1.059 Eval Acc: 0.691\n","Epoch time: 143.0 Total training time: 724.6\n","\n","Epoch: 006 Train Loss: 1.119 Train Acc: 0.693 Eval Loss: 1.022 Eval Acc: 0.699\n","Epoch time: 142.5 Total training time: 867.1\n","\n","Epoch: 007 Train Loss: 1.070 Train Acc: 0.706 Eval Loss: 0.998 Eval Acc: 0.708\n","Epoch time: 142.8 Total training time: 1009.9\n","\n","Epoch: 008 Train Loss: 1.028 Train Acc: 0.718 Eval Loss: 0.977 Eval Acc: 0.712\n","Epoch time: 142.9 Total training time: 1152.8\n","\n","Epoch: 009 Train Loss: 1.000 Train Acc: 0.726 Eval Loss: 0.959 Eval Acc: 0.720\n","Epoch time: 142.6 Total training time: 1295.4\n","\n","Epoch: 010 Train Loss: 0.972 Train Acc: 0.733 Eval Loss: 0.944 Eval Acc: 0.723\n","Epoch time: 142.7 Total training time: 1438.0\n","\n","Epoch: 011 Train Loss: 0.943 Train Acc: 0.742 Eval Loss: 0.936 Eval Acc: 0.726\n","Epoch time: 142.5 Total training time: 1580.5\n","\n","Epoch: 012 Train Loss: 0.921 Train Acc: 0.745 Eval Loss: 0.930 Eval Acc: 0.726\n","Epoch time: 142.5 Total training time: 1723.0\n","\n","Test Accuracy: 0.726\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.87      0.89      0.88       100\n","           1       0.87      0.85      0.86       100\n","           2       0.57      0.62      0.59       100\n","           3       0.52      0.47      0.49       100\n","           4       0.59      0.58      0.59       100\n","           5       0.70      0.73      0.72       100\n","           6       0.76      0.82      0.79       100\n","           7       0.78      0.69      0.73       100\n","           8       0.89      0.88      0.88       100\n","           9       0.85      0.86      0.86       100\n","          10       0.68      0.58      0.63       100\n","          11       0.51      0.54      0.52       100\n","          12       0.79      0.77      0.78       100\n","          13       0.78      0.63      0.70       100\n","          14       0.81      0.78      0.80       100\n","          15       0.68      0.71      0.70       100\n","          16       0.78      0.76      0.77       100\n","          17       0.82      0.84      0.83       100\n","          18       0.69      0.75      0.72       100\n","          19       0.67      0.71      0.69       100\n","          20       0.83      0.85      0.84       100\n","          21       0.77      0.87      0.82       100\n","          22       0.75      0.76      0.76       100\n","          23       0.86      0.82      0.84       100\n","          24       0.83      0.84      0.84       100\n","          25       0.66      0.66      0.66       100\n","          26       0.77      0.68      0.72       100\n","          27       0.56      0.70      0.62       100\n","          28       0.84      0.81      0.82       100\n","          29       0.75      0.75      0.75       100\n","          30       0.60      0.67      0.63       100\n","          31       0.72      0.65      0.68       100\n","          32       0.67      0.70      0.69       100\n","          33       0.80      0.61      0.69       100\n","          34       0.71      0.73      0.72       100\n","          35       0.45      0.37      0.41       100\n","          36       0.74      0.81      0.78       100\n","          37       0.78      0.69      0.73       100\n","          38       0.66      0.60      0.63       100\n","          39       0.88      0.90      0.89       100\n","          40       0.71      0.76      0.73       100\n","          41       0.83      0.86      0.84       100\n","          42       0.79      0.77      0.78       100\n","          43       0.74      0.79      0.76       100\n","          44       0.50      0.50      0.50       100\n","          45       0.57      0.63      0.60       100\n","          46       0.48      0.60      0.54       100\n","          47       0.58      0.60      0.59       100\n","          48       0.85      0.92      0.88       100\n","          49       0.83      0.85      0.84       100\n","          50       0.59      0.48      0.53       100\n","          51       0.72      0.76      0.74       100\n","          52       0.56      0.67      0.61       100\n","          53       0.81      0.96      0.88       100\n","          54       0.79      0.81      0.80       100\n","          55       0.54      0.39      0.45       100\n","          56       0.87      0.91      0.89       100\n","          57       0.87      0.81      0.84       100\n","          58       0.87      0.90      0.89       100\n","          59       0.72      0.52      0.60       100\n","          60       0.80      0.82      0.81       100\n","          61       0.71      0.70      0.71       100\n","          62       0.70      0.75      0.72       100\n","          63       0.66      0.67      0.67       100\n","          64       0.73      0.62      0.67       100\n","          65       0.63      0.67      0.65       100\n","          66       0.76      0.79      0.77       100\n","          67       0.67      0.52      0.58       100\n","          68       0.84      0.92      0.88       100\n","          69       0.86      0.84      0.85       100\n","          70       0.78      0.71      0.74       100\n","          71       0.73      0.79      0.76       100\n","          72       0.47      0.47      0.47       100\n","          73       0.56      0.68      0.62       100\n","          74       0.50      0.53      0.51       100\n","          75       0.88      0.90      0.89       100\n","          76       0.87      0.90      0.89       100\n","          77       0.78      0.69      0.73       100\n","          78       0.62      0.72      0.67       100\n","          79       0.74      0.83      0.78       100\n","          80       0.61      0.62      0.62       100\n","          81       0.70      0.83      0.76       100\n","          82       0.89      0.91      0.90       100\n","          83       0.73      0.66      0.69       100\n","          84       0.65      0.67      0.66       100\n","          85       0.80      0.82      0.81       100\n","          86       0.78      0.76      0.77       100\n","          87       0.83      0.84      0.84       100\n","          88       0.82      0.77      0.79       100\n","          89       0.84      0.83      0.83       100\n","          90       0.81      0.76      0.78       100\n","          91       0.82      0.81      0.81       100\n","          92       0.66      0.66      0.66       100\n","          93       0.69      0.56      0.62       100\n","          94       0.92      0.93      0.93       100\n","          95       0.71      0.67      0.69       100\n","          96       0.58      0.64      0.61       100\n","          97       0.74      0.75      0.75       100\n","          98       0.59      0.47      0.52       100\n","          99       0.79      0.74      0.76       100\n","\n","    accuracy                           0.73     10000\n","   macro avg       0.73      0.73      0.73     10000\n","weighted avg       0.73      0.73      0.73     10000\n","\n","Global Sparsity:\n","0.9872\n","Pruning and Finetuning 5/6\n","Pruning...\n","Test Accuracy: 0.041\n","Global Sparsity:\n","0.9905\n","Fine-tuning...\n","Epoch: 000 Eval Loss: 7.683 Eval Acc: 0.041\n","Epoch: 001 Train Loss: 2.268 Train Acc: 0.414 Eval Loss: 1.642 Eval Acc: 0.548\n","Epoch time: 142.4 Total training time: 153.6\n","\n","Epoch: 002 Train Loss: 1.771 Train Acc: 0.532 Eval Loss: 1.416 Eval Acc: 0.605\n","Epoch time: 142.7 Total training time: 296.3\n","\n","Epoch: 003 Train Loss: 1.595 Train Acc: 0.574 Eval Loss: 1.308 Eval Acc: 0.632\n","Epoch time: 142.5 Total training time: 438.8\n","\n","Epoch: 004 Train Loss: 1.483 Train Acc: 0.602 Eval Loss: 1.234 Eval Acc: 0.650\n","Epoch time: 142.4 Total training time: 581.2\n","\n","Epoch: 005 Train Loss: 1.415 Train Acc: 0.617 Eval Loss: 1.184 Eval Acc: 0.663\n","Epoch time: 142.6 Total training time: 723.8\n","\n","Epoch: 006 Train Loss: 1.356 Train Acc: 0.633 Eval Loss: 1.154 Eval Acc: 0.671\n","Epoch time: 142.6 Total training time: 866.4\n","\n","Epoch: 007 Train Loss: 1.313 Train Acc: 0.645 Eval Loss: 1.128 Eval Acc: 0.679\n","Epoch time: 142.4 Total training time: 1008.8\n","\n","Epoch: 008 Train Loss: 1.261 Train Acc: 0.658 Eval Loss: 1.101 Eval Acc: 0.682\n","Epoch time: 142.4 Total training time: 1151.2\n","\n","Epoch: 009 Train Loss: 1.238 Train Acc: 0.663 Eval Loss: 1.080 Eval Acc: 0.689\n","Epoch time: 142.6 Total training time: 1293.8\n","\n","Epoch: 010 Train Loss: 1.202 Train Acc: 0.671 Eval Loss: 1.065 Eval Acc: 0.692\n","Epoch time: 142.6 Total training time: 1436.4\n","\n","Epoch: 011 Train Loss: 1.177 Train Acc: 0.678 Eval Loss: 1.052 Eval Acc: 0.695\n","Epoch time: 142.6 Total training time: 1579.0\n","\n","Epoch: 012 Train Loss: 1.161 Train Acc: 0.682 Eval Loss: 1.036 Eval Acc: 0.697\n","Epoch time: 142.7 Total training time: 1721.7\n","\n","Epoch: 013 Train Loss: 1.147 Train Acc: 0.685 Eval Loss: 1.029 Eval Acc: 0.699\n","Epoch time: 142.5 Total training time: 1864.2\n","\n","Epoch: 014 Train Loss: 1.122 Train Acc: 0.692 Eval Loss: 1.016 Eval Acc: 0.704\n","Epoch time: 142.7 Total training time: 2006.8\n","\n","Epoch: 015 Train Loss: 1.105 Train Acc: 0.697 Eval Loss: 1.007 Eval Acc: 0.709\n","Epoch time: 142.8 Total training time: 2149.7\n","\n","Epoch: 016 Train Loss: 1.088 Train Acc: 0.700 Eval Loss: 1.003 Eval Acc: 0.708\n","Epoch time: 142.6 Total training time: 2292.3\n","\n","Test Accuracy: 0.708\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.84      0.88      0.86       100\n","           1       0.77      0.82      0.80       100\n","           2       0.60      0.68      0.64       100\n","           3       0.63      0.47      0.54       100\n","           4       0.48      0.52      0.50       100\n","           5       0.79      0.73      0.76       100\n","           6       0.71      0.81      0.76       100\n","           7       0.72      0.67      0.69       100\n","           8       0.91      0.91      0.91       100\n","           9       0.79      0.88      0.83       100\n","          10       0.66      0.49      0.56       100\n","          11       0.51      0.48      0.49       100\n","          12       0.69      0.72      0.70       100\n","          13       0.73      0.55      0.63       100\n","          14       0.69      0.72      0.70       100\n","          15       0.64      0.77      0.70       100\n","          16       0.70      0.74      0.72       100\n","          17       0.77      0.81      0.79       100\n","          18       0.69      0.70      0.69       100\n","          19       0.68      0.77      0.72       100\n","          20       0.78      0.85      0.81       100\n","          21       0.86      0.88      0.87       100\n","          22       0.83      0.70      0.76       100\n","          23       0.91      0.81      0.86       100\n","          24       0.88      0.84      0.86       100\n","          25       0.67      0.64      0.65       100\n","          26       0.69      0.73      0.71       100\n","          27       0.57      0.69      0.63       100\n","          28       0.78      0.78      0.78       100\n","          29       0.65      0.74      0.69       100\n","          30       0.58      0.69      0.63       100\n","          31       0.71      0.63      0.67       100\n","          32       0.69      0.68      0.68       100\n","          33       0.76      0.57      0.65       100\n","          34       0.69      0.74      0.71       100\n","          35       0.51      0.37      0.43       100\n","          36       0.70      0.73      0.72       100\n","          37       0.78      0.68      0.73       100\n","          38       0.57      0.58      0.58       100\n","          39       0.89      0.86      0.87       100\n","          40       0.67      0.58      0.62       100\n","          41       0.82      0.84      0.83       100\n","          42       0.70      0.74      0.72       100\n","          43       0.74      0.74      0.74       100\n","          44       0.51      0.55      0.53       100\n","          45       0.59      0.63      0.61       100\n","          46       0.52      0.56      0.54       100\n","          47       0.67      0.58      0.62       100\n","          48       0.87      0.97      0.92       100\n","          49       0.77      0.85      0.81       100\n","          50       0.51      0.50      0.51       100\n","          51       0.70      0.74      0.72       100\n","          52       0.58      0.65      0.61       100\n","          53       0.77      0.90      0.83       100\n","          54       0.76      0.83      0.79       100\n","          55       0.43      0.44      0.44       100\n","          56       0.87      0.89      0.88       100\n","          57       0.73      0.80      0.77       100\n","          58       0.82      0.87      0.84       100\n","          59       0.60      0.62      0.61       100\n","          60       0.84      0.84      0.84       100\n","          61       0.65      0.75      0.70       100\n","          62       0.71      0.74      0.73       100\n","          63       0.64      0.63      0.64       100\n","          64       0.71      0.53      0.61       100\n","          65       0.59      0.58      0.59       100\n","          66       0.85      0.73      0.78       100\n","          67       0.59      0.52      0.55       100\n","          68       0.88      0.94      0.91       100\n","          69       0.76      0.83      0.79       100\n","          70       0.74      0.73      0.74       100\n","          71       0.72      0.76      0.74       100\n","          72       0.51      0.38      0.43       100\n","          73       0.55      0.54      0.55       100\n","          74       0.48      0.50      0.49       100\n","          75       0.89      0.83      0.86       100\n","          76       0.86      0.88      0.87       100\n","          77       0.68      0.70      0.69       100\n","          78       0.57      0.63      0.60       100\n","          79       0.72      0.83      0.77       100\n","          80       0.61      0.51      0.56       100\n","          81       0.68      0.81      0.74       100\n","          82       0.87      0.87      0.87       100\n","          83       0.77      0.63      0.69       100\n","          84       0.68      0.62      0.65       100\n","          85       0.82      0.80      0.81       100\n","          86       0.73      0.68      0.70       100\n","          87       0.82      0.85      0.83       100\n","          88       0.78      0.72      0.75       100\n","          89       0.78      0.81      0.79       100\n","          90       0.72      0.79      0.76       100\n","          91       0.84      0.81      0.83       100\n","          92       0.71      0.65      0.68       100\n","          93       0.67      0.57      0.62       100\n","          94       0.88      0.92      0.90       100\n","          95       0.72      0.63      0.67       100\n","          96       0.66      0.68      0.67       100\n","          97       0.74      0.76      0.75       100\n","          98       0.58      0.53      0.55       100\n","          99       0.75      0.74      0.74       100\n","\n","    accuracy                           0.71     10000\n","   macro avg       0.71      0.71      0.71     10000\n","weighted avg       0.71      0.71      0.71     10000\n","\n","Global Sparsity:\n","0.9905\n","Pruning and Finetuning 6/6\n","Pruning...\n","Test Accuracy: 0.033\n","Global Sparsity:\n","0.9927\n","Fine-tuning...\n","Epoch: 000 Eval Loss: 6.602 Eval Acc: 0.033\n","Epoch: 001 Train Loss: 2.518 Train Acc: 0.367 Eval Loss: 1.838 Eval Acc: 0.516\n","Epoch time: 142.6 Total training time: 153.4\n","\n","Epoch: 002 Train Loss: 2.012 Train Acc: 0.484 Eval Loss: 1.601 Eval Acc: 0.566\n","Epoch time: 142.6 Total training time: 295.9\n","\n","Epoch: 003 Train Loss: 1.826 Train Acc: 0.525 Eval Loss: 1.474 Eval Acc: 0.595\n","Epoch time: 142.3 Total training time: 438.2\n","\n","Epoch: 004 Train Loss: 1.714 Train Acc: 0.551 Eval Loss: 1.400 Eval Acc: 0.614\n","Epoch time: 142.4 Total training time: 580.6\n","\n","Epoch: 005 Train Loss: 1.635 Train Acc: 0.569 Eval Loss: 1.349 Eval Acc: 0.620\n","Epoch time: 142.4 Total training time: 723.1\n","\n","Epoch: 006 Train Loss: 1.570 Train Acc: 0.586 Eval Loss: 1.310 Eval Acc: 0.632\n","Epoch time: 142.4 Total training time: 865.5\n","\n","Epoch: 007 Train Loss: 1.517 Train Acc: 0.596 Eval Loss: 1.277 Eval Acc: 0.641\n","Epoch time: 142.4 Total training time: 1007.9\n","\n","Epoch: 008 Train Loss: 1.495 Train Acc: 0.601 Eval Loss: 1.251 Eval Acc: 0.645\n","Epoch time: 142.5 Total training time: 1150.4\n","\n","Epoch: 009 Train Loss: 1.455 Train Acc: 0.610 Eval Loss: 1.229 Eval Acc: 0.648\n","Epoch time: 142.4 Total training time: 1292.8\n","\n","Epoch: 010 Train Loss: 1.425 Train Acc: 0.618 Eval Loss: 1.210 Eval Acc: 0.652\n","Epoch time: 142.4 Total training time: 1435.1\n","\n","Epoch: 011 Train Loss: 1.401 Train Acc: 0.623 Eval Loss: 1.194 Eval Acc: 0.658\n","Epoch time: 142.5 Total training time: 1577.7\n","\n","Epoch: 012 Train Loss: 1.381 Train Acc: 0.627 Eval Loss: 1.184 Eval Acc: 0.658\n","Epoch time: 142.6 Total training time: 1720.3\n","\n","Epoch: 013 Train Loss: 1.358 Train Acc: 0.633 Eval Loss: 1.170 Eval Acc: 0.663\n","Epoch time: 142.4 Total training time: 1862.6\n","\n","Epoch: 014 Train Loss: 1.342 Train Acc: 0.637 Eval Loss: 1.156 Eval Acc: 0.668\n","Epoch time: 142.8 Total training time: 2005.4\n","\n","Epoch: 015 Train Loss: 1.322 Train Acc: 0.642 Eval Loss: 1.145 Eval Acc: 0.668\n","Epoch time: 142.6 Total training time: 2148.0\n","\n","Epoch: 016 Train Loss: 1.296 Train Acc: 0.647 Eval Loss: 1.137 Eval Acc: 0.669\n","Epoch time: 142.8 Total training time: 2290.7\n","\n","Test Accuracy: 0.669\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.75      0.86      0.80       100\n","           1       0.79      0.85      0.82       100\n","           2       0.54      0.55      0.54       100\n","           3       0.55      0.38      0.45       100\n","           4       0.41      0.48      0.44       100\n","           5       0.68      0.71      0.70       100\n","           6       0.65      0.78      0.71       100\n","           7       0.71      0.62      0.66       100\n","           8       0.86      0.89      0.88       100\n","           9       0.82      0.81      0.81       100\n","          10       0.56      0.40      0.47       100\n","          11       0.41      0.45      0.43       100\n","          12       0.70      0.80      0.74       100\n","          13       0.62      0.48      0.54       100\n","          14       0.77      0.65      0.71       100\n","          15       0.57      0.69      0.63       100\n","          16       0.67      0.70      0.68       100\n","          17       0.78      0.78      0.78       100\n","          18       0.62      0.66      0.64       100\n","          19       0.65      0.60      0.62       100\n","          20       0.78      0.83      0.81       100\n","          21       0.71      0.83      0.76       100\n","          22       0.66      0.69      0.67       100\n","          23       0.85      0.78      0.81       100\n","          24       0.82      0.84      0.83       100\n","          25       0.67      0.56      0.61       100\n","          26       0.71      0.61      0.66       100\n","          27       0.58      0.71      0.64       100\n","          28       0.80      0.78      0.79       100\n","          29       0.74      0.69      0.72       100\n","          30       0.57      0.64      0.60       100\n","          31       0.70      0.57      0.63       100\n","          32       0.63      0.55      0.59       100\n","          33       0.64      0.59      0.61       100\n","          34       0.64      0.67      0.65       100\n","          35       0.50      0.37      0.43       100\n","          36       0.76      0.79      0.77       100\n","          37       0.65      0.70      0.67       100\n","          38       0.54      0.52      0.53       100\n","          39       0.87      0.86      0.86       100\n","          40       0.61      0.55      0.58       100\n","          41       0.83      0.86      0.85       100\n","          42       0.69      0.68      0.68       100\n","          43       0.73      0.70      0.71       100\n","          44       0.45      0.45      0.45       100\n","          45       0.46      0.51      0.49       100\n","          46       0.55      0.50      0.52       100\n","          47       0.65      0.47      0.55       100\n","          48       0.82      0.93      0.87       100\n","          49       0.75      0.88      0.81       100\n","          50       0.49      0.43      0.46       100\n","          51       0.59      0.66      0.63       100\n","          52       0.53      0.78      0.63       100\n","          53       0.74      0.96      0.84       100\n","          54       0.74      0.81      0.77       100\n","          55       0.39      0.36      0.37       100\n","          56       0.87      0.84      0.85       100\n","          57       0.75      0.73      0.74       100\n","          58       0.82      0.75      0.78       100\n","          59       0.64      0.49      0.55       100\n","          60       0.75      0.79      0.77       100\n","          61       0.67      0.72      0.69       100\n","          62       0.74      0.72      0.73       100\n","          63       0.58      0.55      0.56       100\n","          64       0.57      0.54      0.55       100\n","          65       0.56      0.50      0.53       100\n","          66       0.70      0.78      0.74       100\n","          67       0.58      0.49      0.53       100\n","          68       0.84      0.87      0.85       100\n","          69       0.74      0.78      0.76       100\n","          70       0.76      0.66      0.71       100\n","          71       0.72      0.74      0.73       100\n","          72       0.39      0.35      0.37       100\n","          73       0.54      0.57      0.56       100\n","          74       0.46      0.44      0.45       100\n","          75       0.76      0.83      0.79       100\n","          76       0.82      0.85      0.83       100\n","          77       0.67      0.72      0.69       100\n","          78       0.53      0.56      0.54       100\n","          79       0.69      0.75      0.72       100\n","          80       0.47      0.47      0.47       100\n","          81       0.55      0.69      0.61       100\n","          82       0.83      0.86      0.84       100\n","          83       0.73      0.56      0.63       100\n","          84       0.64      0.60      0.62       100\n","          85       0.77      0.76      0.76       100\n","          86       0.68      0.75      0.71       100\n","          87       0.77      0.81      0.79       100\n","          88       0.71      0.75      0.73       100\n","          89       0.71      0.72      0.71       100\n","          90       0.71      0.67      0.69       100\n","          91       0.78      0.77      0.77       100\n","          92       0.65      0.56      0.60       100\n","          93       0.57      0.54      0.56       100\n","          94       0.87      0.89      0.88       100\n","          95       0.71      0.65      0.68       100\n","          96       0.59      0.60      0.60       100\n","          97       0.70      0.70      0.70       100\n","          98       0.53      0.50      0.51       100\n","          99       0.72      0.76      0.74       100\n","\n","    accuracy                           0.67     10000\n","   macro avg       0.67      0.67      0.67     10000\n","weighted avg       0.67      0.67      0.67     10000\n","\n","Global Sparsity:\n","0.9927\n"]},{"data":{"text/plain":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (2): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (2): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (3): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (2): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (3): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (4): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (5): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (2): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Linear(in_features=512, out_features=100, bias=True)\n",")"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["print(\"Pruning...\")\n","iterative_pruning_finetuning(\n","        model=model,\n","        train_loader=train_loader,\n","        test_loader=test_loader,\n","        device=cuda_device,\n","        learning_rate=learning_rate,\n","        learning_rate_decay=learning_rate_decay,\n","        l1_regularization_strength=l1_regularization_strength,\n","        l2_regularization_strength=l2_regularization_strength,\n","        weight_decay=weight_decay,\n","        conv2d_prune_amount=0.9,\n","        linear_prune_amount=0,\n","        num_iterations=6,           \n","        num_epochs_per_iteration=12,  \n","        #model_filename_prefix=model_filename_prefix,\n","        #model_dir=model_dir,\n","        grouped_pruning=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-30T16:01:02.116917Z","iopub.status.busy":"2024-03-30T16:01:02.116507Z","iopub.status.idle":"2024-03-30T16:01:02.171796Z","shell.execute_reply":"2024-03-30T16:01:02.170718Z","shell.execute_reply.started":"2024-03-30T16:01:02.116884Z"},"trusted":true},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T19:25:41.006599Z","iopub.status.busy":"2024-05-09T19:25:41.006179Z","iopub.status.idle":"2024-05-09T19:25:52.330806Z","shell.execute_reply":"2024-05-09T19:25:52.329575Z","shell.execute_reply.started":"2024-05-09T19:25:41.006562Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.75      0.86      0.80       100\n","           1       0.79      0.85      0.82       100\n","           2       0.54      0.55      0.54       100\n","           3       0.55      0.38      0.45       100\n","           4       0.41      0.48      0.44       100\n","           5       0.68      0.71      0.70       100\n","           6       0.65      0.78      0.71       100\n","           7       0.71      0.62      0.66       100\n","           8       0.86      0.89      0.88       100\n","           9       0.82      0.81      0.81       100\n","          10       0.56      0.40      0.47       100\n","          11       0.41      0.45      0.43       100\n","          12       0.70      0.80      0.74       100\n","          13       0.62      0.48      0.54       100\n","          14       0.77      0.65      0.71       100\n","          15       0.57      0.69      0.63       100\n","          16       0.67      0.70      0.68       100\n","          17       0.78      0.78      0.78       100\n","          18       0.62      0.66      0.64       100\n","          19       0.65      0.60      0.62       100\n","          20       0.78      0.83      0.81       100\n","          21       0.71      0.83      0.76       100\n","          22       0.66      0.69      0.67       100\n","          23       0.85      0.78      0.81       100\n","          24       0.82      0.84      0.83       100\n","          25       0.67      0.56      0.61       100\n","          26       0.71      0.61      0.66       100\n","          27       0.58      0.71      0.64       100\n","          28       0.80      0.78      0.79       100\n","          29       0.74      0.69      0.72       100\n","          30       0.57      0.64      0.60       100\n","          31       0.70      0.57      0.63       100\n","          32       0.63      0.55      0.59       100\n","          33       0.64      0.59      0.61       100\n","          34       0.64      0.67      0.65       100\n","          35       0.50      0.37      0.43       100\n","          36       0.76      0.79      0.77       100\n","          37       0.65      0.70      0.67       100\n","          38       0.54      0.52      0.53       100\n","          39       0.87      0.86      0.86       100\n","          40       0.61      0.55      0.58       100\n","          41       0.83      0.86      0.85       100\n","          42       0.69      0.68      0.68       100\n","          43       0.73      0.70      0.71       100\n","          44       0.45      0.45      0.45       100\n","          45       0.46      0.51      0.49       100\n","          46       0.55      0.50      0.52       100\n","          47       0.65      0.47      0.55       100\n","          48       0.82      0.93      0.87       100\n","          49       0.75      0.88      0.81       100\n","          50       0.49      0.43      0.46       100\n","          51       0.59      0.66      0.63       100\n","          52       0.53      0.78      0.63       100\n","          53       0.74      0.96      0.84       100\n","          54       0.74      0.81      0.77       100\n","          55       0.39      0.36      0.37       100\n","          56       0.87      0.84      0.85       100\n","          57       0.75      0.73      0.74       100\n","          58       0.82      0.75      0.78       100\n","          59       0.64      0.49      0.55       100\n","          60       0.75      0.79      0.77       100\n","          61       0.67      0.72      0.69       100\n","          62       0.74      0.72      0.73       100\n","          63       0.58      0.55      0.56       100\n","          64       0.57      0.54      0.55       100\n","          65       0.56      0.50      0.53       100\n","          66       0.70      0.78      0.74       100\n","          67       0.58      0.49      0.53       100\n","          68       0.84      0.87      0.85       100\n","          69       0.74      0.78      0.76       100\n","          70       0.76      0.66      0.71       100\n","          71       0.72      0.74      0.73       100\n","          72       0.39      0.35      0.37       100\n","          73       0.54      0.57      0.56       100\n","          74       0.46      0.44      0.45       100\n","          75       0.76      0.83      0.79       100\n","          76       0.82      0.85      0.83       100\n","          77       0.67      0.72      0.69       100\n","          78       0.53      0.56      0.54       100\n","          79       0.69      0.75      0.72       100\n","          80       0.47      0.47      0.47       100\n","          81       0.55      0.69      0.61       100\n","          82       0.83      0.86      0.84       100\n","          83       0.73      0.56      0.63       100\n","          84       0.64      0.60      0.62       100\n","          85       0.77      0.76      0.76       100\n","          86       0.68      0.75      0.71       100\n","          87       0.77      0.81      0.79       100\n","          88       0.71      0.75      0.73       100\n","          89       0.71      0.72      0.71       100\n","          90       0.71      0.67      0.69       100\n","          91       0.78      0.77      0.77       100\n","          92       0.65      0.56      0.60       100\n","          93       0.57      0.54      0.56       100\n","          94       0.87      0.89      0.88       100\n","          95       0.71      0.65      0.68       100\n","          96       0.59      0.60      0.60       100\n","          97       0.70      0.70      0.70       100\n","          98       0.53      0.50      0.51       100\n","          99       0.72      0.76      0.74       100\n","\n","    accuracy                           0.67     10000\n","   macro avg       0.67      0.67      0.67     10000\n","weighted avg       0.67      0.67      0.67     10000\n","\n"]}],"source":["classification_report = create_classification_report(\n","        model=model, test_loader=test_loader, device=cuda_device)\n","print(classification_report)"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T19:26:30.876473Z","iopub.status.busy":"2024-05-09T19:26:30.875823Z","iopub.status.idle":"2024-05-09T19:26:30.887870Z","shell.execute_reply":"2024-05-09T19:26:30.886984Z","shell.execute_reply.started":"2024-05-09T19:26:30.876442Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Global Sparsity:\n","0.9927\n"]}],"source":["num_zeros, num_elements, sparsity = measure_global_sparsity(model, conv2d_use_mask=True)\n","\n","#print(\"Test Accuracy: {:.3f}\".format(eval_accuracy))\n","#print(\"Classification Report:\")\n","#print(classification_report)\n","print(\"Global Sparsity:\")\n","print(\"{:.4f}\".format(sparsity))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_model()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T16:11:17.62069Z","iopub.status.busy":"2024-05-06T16:11:17.619941Z","iopub.status.idle":"2024-05-06T16:11:17.949634Z","shell.execute_reply":"2024-05-06T16:11:17.948818Z","shell.execute_reply.started":"2024-05-06T16:11:17.620661Z"},"trusted":true},"outputs":[],"source":["checkpoint = torch.load(\"/kaggle/working/checkpoint_epoch30.pth.tar\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T16:35:01.338792Z","iopub.status.busy":"2024-05-06T16:35:01.33841Z","iopub.status.idle":"2024-05-06T16:35:03.50776Z","shell.execute_reply":"2024-05-06T16:35:03.506791Z","shell.execute_reply.started":"2024-05-06T16:35:01.338766Z"},"trusted":true},"outputs":[],"source":["a = create_model(num_classes=num_classes)\n","parameters_to_prune = []\n","    \n","a.fc = torch.nn.Linear(a.fc.in_features, 100)\n","for module_name, module in a.named_modules():\n","    if isinstance(module, torch.nn.Conv2d):\n","        parameters_to_prune.append((module, \"weight\"))\n","prune.global_unstructured(\n","                        parameters_to_prune,\n","                        pruning_method=prune.L1Unstructured,\n","                        amount=0,\n","                        )\n","\n","    \n","a = load_model(model=a,\n","                    model_filepath=\"/kaggle/working/checkpoint_epoch30.pth.tar\",\n","                    device=cuda_device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T16:35:17.711379Z","iopub.status.busy":"2024-05-06T16:35:17.710979Z","iopub.status.idle":"2024-05-06T16:35:29.491346Z","shell.execute_reply":"2024-05-06T16:35:29.490096Z","shell.execute_reply.started":"2024-05-06T16:35:17.711348Z"},"trusted":true},"outputs":[],"source":["classification_report = create_classification_report(\n","        model=a, test_loader=test_loader, device=cuda_device)\n","print(classification_report)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T16:35:49.224635Z","iopub.status.busy":"2024-05-06T16:35:49.223574Z","iopub.status.idle":"2024-05-06T16:35:49.23524Z","shell.execute_reply":"2024-05-06T16:35:49.234219Z","shell.execute_reply.started":"2024-05-06T16:35:49.224593Z"},"trusted":true},"outputs":[],"source":["num_zeros, num_elements, sparsity = measure_global_sparsity(a, conv2d_use_mask=True)\n","print(sparsity)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"modelInstanceId":37215,"sourceId":44307,"sourceType":"modelInstanceVersion"},{"modelInstanceId":37901,"sourceId":45173,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":true,"modelInstanceId":37901,"sourceId":45776,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
