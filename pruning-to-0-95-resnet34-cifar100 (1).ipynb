{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T11:59:48.494819Z","iopub.status.busy":"2024-05-07T11:59:48.494484Z","iopub.status.idle":"2024-05-07T11:59:55.773316Z","shell.execute_reply":"2024-05-07T11:59:55.772454Z","shell.execute_reply.started":"2024-05-07T11:59:48.494794Z"},"trusted":true},"outputs":[],"source":["import os\n","import random\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","from torchvision import datasets, transforms\n","\n","import time\n","import copy\n","\n","import numpy as np\n","\n","import sklearn.metrics\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T11:59:57.437552Z","iopub.status.busy":"2024-05-07T11:59:57.437058Z","iopub.status.idle":"2024-05-07T11:59:57.442934Z","shell.execute_reply":"2024-05-07T11:59:57.441975Z","shell.execute_reply.started":"2024-05-07T11:59:57.437521Z"},"trusted":true},"outputs":[],"source":["def set_random_seeds(random_seed=0):\n","\n","    torch.manual_seed(random_seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    np.random.seed(random_seed)\n","    random.seed(random_seed)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T11:59:58.199067Z","iopub.status.busy":"2024-05-07T11:59:58.198486Z","iopub.status.idle":"2024-05-07T11:59:59.200339Z","shell.execute_reply":"2024-05-07T11:59:59.199437Z","shell.execute_reply.started":"2024-05-07T11:59:58.199034Z"},"trusted":true},"outputs":[],"source":["#import os\n","#import time\n","import math\n","#import random\n","#import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","import glob\n","\n","import matplotlib.pyplot as plt\n","from PIL import Image, ImageEnhance, ImageOps\n","\n","from tqdm import tqdm, tqdm_notebook\n","\n","import torch\n","from torch import nn, cuda\n","from torch.autograd import Variable \n","import torch.nn.functional as F\n","import torchvision as vision\n","import torchvision.models as models\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim import Adam, SGD, Optimizer\n","from torch.optim.lr_scheduler import _LRScheduler, CosineAnnealingLR, ReduceLROnPlateau\n","\n","from sklearn.metrics import f1_score\n","\n","class CIFAR10Policy(object):\n","    \"\"\" Randomly choose one of the best 25 Sub-policies on CIFAR10.\n","        Example:\n","        >>> policy = CIFAR10Policy()\n","        >>> transformed = policy(image)\n","        Example as a PyTorch Transform:\n","        >>> transform=transforms.Compose([\n","        >>>     transforms.Resize(256),\n","        >>>     CIFAR10Policy(),\n","        >>>     transforms.ToTensor()])\n","    \"\"\"\n","    def __init__(self, fillcolor=(128, 128, 128)):\n","        self.policies = [\n","            SubPolicy(0.1, \"invert\", 7, 0.2, \"contrast\", 6, fillcolor),\n","            SubPolicy(0.7, \"rotate\", 2, 0.3, \"translateX\", 9, fillcolor),\n","            SubPolicy(0.8, \"sharpness\", 1, 0.9, \"sharpness\", 3, fillcolor),\n","            SubPolicy(0.5, \"shearY\", 8, 0.7, \"translateY\", 9, fillcolor),\n","            SubPolicy(0.5, \"autocontrast\", 8, 0.9, \"equalize\", 2, fillcolor),\n","\n","            SubPolicy(0.2, \"shearY\", 7, 0.3, \"posterize\", 7, fillcolor),\n","            SubPolicy(0.4, \"color\", 3, 0.6, \"brightness\", 7, fillcolor),\n","            SubPolicy(0.3, \"sharpness\", 9, 0.7, \"brightness\", 9, fillcolor),\n","            SubPolicy(0.6, \"equalize\", 5, 0.5, \"equalize\", 1, fillcolor),\n","            SubPolicy(0.6, \"contrast\", 7, 0.6, \"sharpness\", 5, fillcolor),\n","\n","            SubPolicy(0.7, \"color\", 7, 0.5, \"translateX\", 8, fillcolor),\n","            SubPolicy(0.3, \"equalize\", 7, 0.4, \"autocontrast\", 8, fillcolor),\n","            SubPolicy(0.4, \"translateY\", 3, 0.2, \"sharpness\", 6, fillcolor),\n","            SubPolicy(0.9, \"brightness\", 6, 0.2, \"color\", 8, fillcolor),\n","            SubPolicy(0.5, \"solarize\", 2, 0.0, \"invert\", 3, fillcolor),\n","\n","            SubPolicy(0.2, \"equalize\", 0, 0.6, \"autocontrast\", 0, fillcolor),\n","            SubPolicy(0.2, \"equalize\", 8, 0.8, \"equalize\", 4, fillcolor),\n","            SubPolicy(0.9, \"color\", 9, 0.6, \"equalize\", 6, fillcolor),\n","            SubPolicy(0.8, \"autocontrast\", 4, 0.2, \"solarize\", 8, fillcolor),\n","            SubPolicy(0.1, \"brightness\", 3, 0.7, \"color\", 0, fillcolor),\n","\n","            SubPolicy(0.4, \"solarize\", 5, 0.9, \"autocontrast\", 3, fillcolor),\n","            SubPolicy(0.9, \"translateY\", 9, 0.7, \"translateY\", 9, fillcolor),\n","            SubPolicy(0.9, \"autocontrast\", 2, 0.8, \"solarize\", 3, fillcolor),\n","            SubPolicy(0.8, \"equalize\", 8, 0.1, \"invert\", 3, fillcolor),\n","            SubPolicy(0.7, \"translateY\", 9, 0.9, \"autocontrast\", 1, fillcolor)\n","        ]\n","\n","\n","    def __call__(self, img):\n","        policy_idx = random.randint(0, len(self.policies) - 1)\n","        return self.policies[policy_idx](img)\n","\n","    def __repr__(self):\n","        return \"AutoAugment CIFAR10 Policy\"\n","\n","\n","class SubPolicy(object):\n","    def __init__(self, p1, operation1, magnitude_idx1, p2, operation2, magnitude_idx2, fillcolor=(128, 128, 128)):\n","        ranges = {\n","            \"shearX\": np.linspace(0, 0.3, 10),\n","            \"shearY\": np.linspace(0, 0.3, 10),\n","            \"translateX\": np.linspace(0, 150 / 331, 10),\n","            \"translateY\": np.linspace(0, 150 / 331, 10),\n","            \"rotate\": np.linspace(0, 30, 10),\n","            \"color\": np.linspace(0.0, 0.9, 10),\n","            \"posterize\": np.round(np.linspace(8, 4, 10), 0).astype(int),\n","            \"solarize\": np.linspace(256, 0, 10),\n","            \"contrast\": np.linspace(0.0, 0.9, 10),\n","            \"sharpness\": np.linspace(0.0, 0.9, 10),\n","            \"brightness\": np.linspace(0.0, 0.9, 10),\n","            \"autocontrast\": [0] * 10,\n","            \"equalize\": [0] * 10,\n","            \"invert\": [0] * 10\n","        }\n","\n","        # from https://stackoverflow.com/questions/5252170/specify-image-filling-color-when-rotating-in-python-with-pil-and-setting-expand\n","        def rotate_with_fill(img, magnitude):\n","            rot = img.convert(\"RGBA\").rotate(magnitude)\n","            return Image.composite(rot, Image.new(\"RGBA\", rot.size, (128,) * 4), rot).convert(img.mode)\n","\n","        func = {\n","            \"shearX\": lambda img, magnitude: img.transform(\n","                img.size, Image.AFFINE, (1, magnitude * random.choice([-1, 1]), 0, 0, 1, 0),\n","                Image.BICUBIC, fillcolor=fillcolor),\n","            \"shearY\": lambda img, magnitude: img.transform(\n","                img.size, Image.AFFINE, (1, 0, 0, magnitude * random.choice([-1, 1]), 1, 0),\n","                Image.BICUBIC, fillcolor=fillcolor),\n","            \"translateX\": lambda img, magnitude: img.transform(\n","                img.size, Image.AFFINE, (1, 0, magnitude * img.size[0] * random.choice([-1, 1]), 0, 1, 0),\n","                fillcolor=fillcolor),\n","            \"translateY\": lambda img, magnitude: img.transform(\n","                img.size, Image.AFFINE, (1, 0, 0, 0, 1, magnitude * img.size[1] * random.choice([-1, 1])),\n","                fillcolor=fillcolor),\n","            \"rotate\": lambda img, magnitude: rotate_with_fill(img, magnitude),\n","            # \"rotate\": lambda img, magnitude: img.rotate(magnitude * random.choice([-1, 1])),\n","            \"color\": lambda img, magnitude: ImageEnhance.Color(img).enhance(1 + magnitude * random.choice([-1, 1])),\n","            \"posterize\": lambda img, magnitude: ImageOps.posterize(img, magnitude),\n","            \"solarize\": lambda img, magnitude: ImageOps.solarize(img, magnitude),\n","            \"contrast\": lambda img, magnitude: ImageEnhance.Contrast(img).enhance(\n","                1 + magnitude * random.choice([-1, 1])),\n","            \"sharpness\": lambda img, magnitude: ImageEnhance.Sharpness(img).enhance(\n","                1 + magnitude * random.choice([-1, 1])),\n","            \"brightness\": lambda img, magnitude: ImageEnhance.Brightness(img).enhance(\n","                1 + magnitude * random.choice([-1, 1])),\n","            \"autocontrast\": lambda img, magnitude: ImageOps.autocontrast(img),\n","            \"equalize\": lambda img, magnitude: ImageOps.equalize(img),\n","            \"invert\": lambda img, magnitude: ImageOps.invert(img)\n","        }\n","\n","        # self.name = \"{}_{:.2f}_and_{}_{:.2f}\".format(\n","        #     operation1, ranges[operation1][magnitude_idx1],\n","        #     operation2, ranges[operation2][magnitude_idx2])\n","        self.p1 = p1\n","        self.operation1 = func[operation1]\n","        self.magnitude1 = ranges[operation1][magnitude_idx1]\n","        self.p2 = p2\n","        self.operation2 = func[operation2]\n","        self.magnitude2 = ranges[operation2][magnitude_idx2]\n","\n","\n","    def __call__(self, img):\n","        if random.random() < self.p1: img = self.operation1(img, self.magnitude1)\n","        if random.random() < self.p2: img = self.operation2(img, self.magnitude2)\n","        return img\n","  \n","\n","class TestDataset(Dataset):\n","    def __init__(self, df, mode='test', transforms=None):\n","        self.df = df\n","        self.mode = mode\n","        self.transform = transforms[self.mode]\n","        \n","    def __len__(self):\n","        return len(self.df)\n","    \n","    def __getitem__(self, idx):\n","        \n","        image = Image.open(TEST_IMAGE_PATH / self.df[idx]).convert(\"RGB\")\n","        \n","        if self.transform:\n","            image = self.transform(image)\n","            \n","        return image"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T11:59:59.202536Z","iopub.status.busy":"2024-05-07T11:59:59.202092Z","iopub.status.idle":"2024-05-07T11:59:59.212786Z","shell.execute_reply":"2024-05-07T11:59:59.211876Z","shell.execute_reply.started":"2024-05-07T11:59:59.202509Z"},"trusted":true},"outputs":[],"source":["def prepare_dataloader(num_workers=0,\n","                       train_batch_size=128,\n","                       eval_batch_size=256,\n","                       mean=(0.4914, 0.4822, 0.4466),\n","                       stdev=(0.2412, 0.2377, 0.2563)):\n","\n","    train_transform = transforms.Compose([\n","        torchvision.transforms.Resize((224,224)),\n","        CIFAR10Policy(),\n","        torchvision.transforms.ToTensor(),\n","        torchvision.transforms.Normalize(mean=mean, std=stdev)\n","    ])\n","\n","    test_transform = transforms.Compose([\n","        torchvision.transforms.Resize((224,224)),\n","        torchvision.transforms.ToTensor(),\n","        torchvision.transforms.Normalize(mean=torch.tensor(mean), std=stdev)\n","    ])\n","\n","    train_set = torchvision.datasets.CIFAR100(root=\"data\",\n","                                             train=True,\n","                                             download=True,\n","                                             transform=train_transform)\n","\n","    test_set = torchvision.datasets.CIFAR100(root=\"data\",\n","                                            train=False,\n","                                            download=True,\n","                                            transform=test_transform)\n","\n","    train_sampler = torch.utils.data.RandomSampler(train_set)\n","    test_sampler = torch.utils.data.SequentialSampler(test_set)\n","\n","    train_loader = torch.utils.data.DataLoader(dataset=train_set,\n","                                               batch_size=train_batch_size,\n","                                               #shuffle=True,\n","                                               sampler=train_sampler,\n","                                               num_workers=num_workers,\n","                                               pin_memory=True\n","                                              )\n","\n","    test_loader = torch.utils.data.DataLoader(dataset=test_set,\n","                                              batch_size=eval_batch_size,\n","                                              #shuffle=False,\n","                                              sampler=test_sampler,\n","                                              num_workers=num_workers,\n","                                              pin_memory=True\n","                                             )\n","\n","    classes = train_set.classes\n","\n","    return train_loader, test_loader, classes"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T11:59:59.315471Z","iopub.status.busy":"2024-05-07T11:59:59.315087Z","iopub.status.idle":"2024-05-07T11:59:59.335160Z","shell.execute_reply":"2024-05-07T11:59:59.334211Z","shell.execute_reply.started":"2024-05-07T11:59:59.315445Z"},"trusted":true},"outputs":[],"source":["def train_model(model,\n","                train_loader,\n","                test_loader,\n","                device,\n","                #model_dir,\n","                #model_filename,\n","                l1_regularization_strength=0,\n","                l2_regularization_strength=0,\n","                weight_decay=5e-4,\n","                learning_rate=1e-4,\n","                num_epochs=200,\n","                checkpoint_epochs=10\n","                ):\n","\n","    \n","    start = time.time()\n","    criterion = nn.CrossEntropyLoss()\n","\n","    model.to(device)\n","\n","    \n","    #optimizer = optim.SGD(model.parameters(),\n","    #                      lr=learning_rate,\n","    #                      momentum=0.9,\n","    #                      weight_decay=l2_regularization_strength)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n","    # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=500)\n","    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n","                                                     milestones=[19, 28, 36],\n","                                                     gamma=0.1,\n","                                                     last_epoch=-1)\n","    # optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n","\n","    # Evaluation\n","    model.eval()\n","    eval_loss, eval_accuracy = evaluate_model(model=model,\n","                                              test_loader=test_loader,\n","                                              device=device,\n","                                              criterion=criterion)\n","    print(\"Epoch: {:03d} Eval Loss: {:.3f} Eval Acc: {:.3f}\".format(\n","        0, eval_loss, eval_accuracy))\n","\n","    for epoch in range(1, num_epochs+1):\n","        epoch_start = time.time()\n","        # Training\n","        model.train()\n","\n","        running_loss = 0\n","        running_corrects = 0\n","\n","        for inputs, labels in train_loader:\n","\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","\n","            # zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","            # forward + backward + optimize\n","            outputs = model(inputs)\n","            _, preds = torch.max(outputs, 1)\n","            loss = criterion(outputs, labels)\n","\n","            l1_reg = torch.tensor(0.).to(device)\n","            for module in model.modules():\n","                mask = None\n","                weight = None\n","                for name, buffer in module.named_buffers():\n","                    if name == \"weight_mask\":\n","                        mask = buffer\n","                for name, param in module.named_parameters():\n","                    if name == \"weight_orig\":\n","                        weight = param\n","                \n","                if mask is not None and weight is not None:\n","                    l1_reg += torch.norm(mask * weight, 1)\n","\n","            loss += l1_regularization_strength * l1_reg \n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            \n","            running_loss += loss.item() * inputs.size(0)\n","            running_corrects += torch.sum(preds == labels.data)\n","\n","        train_loss = running_loss / len(train_loader.dataset)\n","        train_accuracy = running_corrects / len(train_loader.dataset)\n","\n","        # Evaluation\n","        model.eval()\n","        eval_loss, eval_accuracy = evaluate_model(model=model,\n","                                                  test_loader=test_loader,\n","                                                  device=device,\n","                                                  criterion=criterion)\n","        if epoch % checkpoint_epochs == 0:\n","            torch.save({\n","                'epoch': epoch,\n","                'state_dict': model.state_dict(),\n","                'optimizer': optimizer.state_dict(),\n","            }, f'./checkpoint_epoch{epoch}.pth.tar')\n","\n","        \n","        scheduler.step()\n","        elapsed = time.time()\n","        print(\n","            \"Epoch: {:03d} Train Loss: {:.3f} Train Acc: {:.3f} Eval Loss: {:.3f} Eval Acc: {:.3f}\"\n","            .format(epoch, train_loss, train_accuracy, eval_loss,\n","                    eval_accuracy))\n","        print(f'Epoch time: {elapsed-epoch_start:.1f} Total training time: {elapsed-start:.1f}')\n","        print()\n","        #torch.cuda.empty_cache()\n","\n","    return model"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T12:00:00.067651Z","iopub.status.busy":"2024-05-07T12:00:00.067320Z","iopub.status.idle":"2024-05-07T12:00:00.081597Z","shell.execute_reply":"2024-05-07T12:00:00.080631Z","shell.execute_reply.started":"2024-05-07T12:00:00.067627Z"},"trusted":true},"outputs":[],"source":["def train(net, train_dataloader, valid_dataloader, criterion, optimizer, scheduler=None, epochs=10, device='cpu', checkpoint_epochs=10):\n","    start = time.time()\n","    print(f'Training for {epochs} epochs on {device}')\n","    \n","    for epoch in range(1,epochs+1):\n","        epoch_start = time.time()\n","        print(f\"Epoch {epoch}/{epochs}\")\n","        \n","        net.train()  \n","        train_loss = torch.tensor(0., device=device)  \n","        train_accuracy = torch.tensor(0., device=device)\n","        for X, y in train_dataloader:\n","            X = X.to(device)\n","            y = y.to(device)\n","            preds = net(X)\n","            loss = criterion(preds, y)\n","            \n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            \n","            with torch.no_grad():\n","                train_loss += loss * train_dataloader.batch_size\n","                train_accuracy += (torch.argmax(preds, dim=1) == y).sum()\n","        \n","        if valid_dataloader is not None:\n","            net.eval()  \n","            valid_loss = torch.tensor(0., device=device)\n","            valid_accuracy = torch.tensor(0., device=device)\n","            with torch.no_grad():\n","                for X, y in valid_dataloader:\n","                    X = X.to(device)\n","                    y = y.to(device)\n","                    preds = net(X)\n","                    loss = criterion(preds, y)\n","\n","                    valid_loss += loss * valid_dataloader.batch_size\n","                    valid_accuracy += (torch.argmax(preds, dim=1) == y).sum()\n","        \n","        if scheduler is not None: \n","            scheduler.step()\n","            \n","        print(f'Training loss: {train_loss/len(train_dataloader.dataset):.2f}')\n","        print(f'Training accuracy: {100*train_accuracy/len(train_dataloader.dataset):.2f}')\n","        \n","        if valid_dataloader is not None:\n","            print(f'Valid loss: {valid_loss/len(valid_dataloader.dataset):.2f}')\n","            print(f'Valid accuracy: {100*valid_accuracy/len(valid_dataloader.dataset):.2f}')\n","        \n","        if epoch % checkpoint_epochs == 0:\n","            torch.save({\n","                'epoch': epoch,\n","                'state_dict': net.state_dict(),\n","                'optimizer': optimizer.state_dict(),\n","            }, f'./checkpoint_epoch{epoch}.pth.tar')\n","        elapsed = time.time()\n","        print(f'Epoch time: {elapsed-epoch_start:.1f} Total training time: {elapsed-start:.1f}')\n","        print()\n","    \n","    end = time.time()\n","    print(f'Total training time: {end-start:.1f} seconds')\n","    return net"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T12:00:00.574399Z","iopub.status.busy":"2024-05-07T12:00:00.574053Z","iopub.status.idle":"2024-05-07T12:00:00.582895Z","shell.execute_reply":"2024-05-07T12:00:00.581919Z","shell.execute_reply.started":"2024-05-07T12:00:00.574374Z"},"trusted":true},"outputs":[],"source":["def measure_module_sparsity(module, weight=True, bias=False, use_mask=False):\n","\n","    num_zeros = 0\n","    num_elements = 0\n","\n","    if use_mask == True:\n","        for buffer_name, buffer in module.named_buffers():\n","            if \"weight_mask\" in buffer_name and weight == True:\n","                num_zeros += torch.sum(buffer == 0).item()\n","                num_elements += buffer.nelement()\n","            if \"bias_mask\" in buffer_name and bias == True:\n","                num_zeros += torch.sum(buffer == 0).item()\n","                num_elements += buffer.nelement()\n","    else:\n","        for param_name, param in module.named_parameters():\n","            if \"weight\" in param_name and weight == True:\n","                num_zeros += torch.sum(param == 0).item()\n","                num_elements += param.nelement()\n","            if \"bias\" in param_name and bias == True:\n","                num_zeros += torch.sum(param == 0).item()\n","                num_elements += param.nelement()\n","\n","    sparsity = num_zeros / num_elements\n","\n","    return num_zeros, num_elements, sparsity"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T12:00:01.107315Z","iopub.status.busy":"2024-05-07T12:00:01.106947Z","iopub.status.idle":"2024-05-07T12:00:01.114686Z","shell.execute_reply":"2024-05-07T12:00:01.113742Z","shell.execute_reply.started":"2024-05-07T12:00:01.107288Z"},"trusted":true},"outputs":[],"source":["def measure_global_sparsity(model,\n","                            weight=True,\n","                            bias=False,\n","                            conv2d_use_mask=False,\n","                            linear_use_mask=False):\n","\n","    num_zeros = 0\n","    num_elements = 0\n","\n","    for module_name, module in model.named_modules():\n","\n","        if isinstance(module, torch.nn.Conv2d):\n","\n","            module_num_zeros, module_num_elements, _ = measure_module_sparsity(\n","                module, weight=weight, bias=bias, use_mask=conv2d_use_mask)\n","            num_zeros += module_num_zeros\n","            num_elements += module_num_elements\n","\n","        elif isinstance(module, torch.nn.Linear):\n","\n","            module_num_zeros, module_num_elements, _ = measure_module_sparsity(\n","                module, weight=weight, bias=bias, use_mask=linear_use_mask)\n","            num_zeros += module_num_zeros\n","            num_elements += module_num_elements\n","\n","    sparsity = num_zeros / num_elements\n","\n","    return num_zeros, num_elements, sparsity"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T12:00:01.589617Z","iopub.status.busy":"2024-05-07T12:00:01.588695Z","iopub.status.idle":"2024-05-07T12:00:01.596872Z","shell.execute_reply":"2024-05-07T12:00:01.595849Z","shell.execute_reply.started":"2024-05-07T12:00:01.589580Z"},"trusted":true},"outputs":[],"source":["def evaluate_model(model, test_loader, device, criterion=None):\n","\n","    model.eval()\n","    model.to(device)\n","\n","    running_loss = 0\n","    running_corrects = 0\n","\n","    for inputs, labels in test_loader:\n","\n","        inputs = inputs.to(device)\n","        labels = labels.to(device)\n","\n","        outputs = model(inputs)\n","        _, preds = torch.max(outputs, 1)\n","\n","        if criterion is not None:\n","            loss = criterion(outputs, labels).item()\n","        else:\n","            loss = 0\n","\n","        # statistics\n","        running_loss += loss * inputs.size(0)\n","        running_corrects += torch.sum(preds == labels.data)\n","        \n","        #torch.cuda.empty_cache()\n","\n","    eval_loss = running_loss / len(test_loader.dataset)\n","    eval_accuracy = running_corrects / len(test_loader.dataset)\n","    \n","\n","    return eval_loss, eval_accuracy"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T12:00:02.052341Z","iopub.status.busy":"2024-05-07T12:00:02.051618Z","iopub.status.idle":"2024-05-07T12:00:02.059207Z","shell.execute_reply":"2024-05-07T12:00:02.058026Z","shell.execute_reply.started":"2024-05-07T12:00:02.052308Z"},"trusted":true},"outputs":[],"source":["def create_classification_report(model, device, test_loader):\n","\n","    model.eval()\n","    model.to(device)\n","\n","    y_pred = []\n","    y_true = []\n","\n","    with torch.no_grad():\n","        for data in test_loader:\n","            y_true += data[1].numpy().tolist()\n","            images, _ = data[0].to(device), data[1].to(device)\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            y_pred += predicted.cpu().numpy().tolist()\n","\n","    classification_report = sklearn.metrics.classification_report(\n","        y_true=y_true, y_pred=y_pred)\n","\n","    return classification_report"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T12:00:29.015153Z","iopub.status.busy":"2024-05-07T12:00:29.014439Z","iopub.status.idle":"2024-05-07T12:00:29.020350Z","shell.execute_reply":"2024-05-07T12:00:29.019256Z","shell.execute_reply.started":"2024-05-07T12:00:29.015120Z"},"trusted":true},"outputs":[],"source":["def save_model(model, model_dir, model_filename):\n","\n","    if not os.path.exists(model_dir):\n","        os.makedirs(model_dir)\n","    model_filepath = os.path.join(model_dir, model_filename)\n","    torch.save(model.state_dict(), model_filepath)\n"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T12:00:30.340296Z","iopub.status.busy":"2024-05-07T12:00:30.339632Z","iopub.status.idle":"2024-05-07T12:00:30.345134Z","shell.execute_reply":"2024-05-07T12:00:30.344120Z","shell.execute_reply.started":"2024-05-07T12:00:30.340266Z"},"trusted":true},"outputs":[],"source":["def load_model(model, model_filepath, device):\n","\n","    model.load_state_dict(torch.load(model_filepath, map_location=device)['state_dict'])\n","\n","    return model"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T12:00:31.147732Z","iopub.status.busy":"2024-05-07T12:00:31.147379Z","iopub.status.idle":"2024-05-07T12:00:31.153226Z","shell.execute_reply":"2024-05-07T12:00:31.152268Z","shell.execute_reply.started":"2024-05-07T12:00:31.147705Z"},"trusted":true},"outputs":[],"source":["def create_model(num_classes=10, model_func=torchvision.models.resnet34):\n","\n","    \n","    model = model_func(num_classes=num_classes, pretrained=False)\n","\n","    \n","\n","    return model"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T12:00:46.473073Z","iopub.status.busy":"2024-05-07T12:00:46.472674Z","iopub.status.idle":"2024-05-07T12:00:46.491897Z","shell.execute_reply":"2024-05-07T12:00:46.490909Z","shell.execute_reply.started":"2024-05-07T12:00:46.473041Z"},"trusted":true},"outputs":[],"source":["def iterative_pruning_finetuning(model,\n","                                 train_loader,\n","                                 test_loader,\n","                                 device,\n","                                 learning_rate,\n","                                 l1_regularization_strength=0,\n","                                 l2_regularization_strength=0,\n","                                 weight_decay=5e-4,\n","                                 learning_rate_decay=0.6,\n","                                 conv2d_prune_amount=0.4,\n","                                 linear_prune_amount=0.2,\n","                                 num_iterations=10,\n","                                 num_epochs_per_iteration=10,\n","                                 #model_filename_prefix=\"pruned_model\",\n","                                 #model_dir=\"saved_models\",\n","                                 grouped_pruning=False):\n","\n","    conv2d_one_iter_prune_amount = 1 - (1 - conv2d_prune_amount)**(1/num_iterations)\n","    linear_one_iter_prune_amount = 1 - (1 - linear_prune_amount)**(1/num_iterations)\n","    for i in range(num_iterations):\n","\n","        print(\"Pruning and Finetuning {}/{}\".format(i + 1, num_iterations))\n","\n","        print(\"Pruning...\")\n","\n","        if grouped_pruning == True:\n","            \n","            parameters_to_prune = []\n","            for module_name, module in model.named_modules():\n","                if isinstance(module, torch.nn.Conv2d):\n","                    parameters_to_prune.append((module, \"weight\"))\n","            prune.global_unstructured(\n","                parameters_to_prune,\n","                pruning_method=prune.L1Unstructured,\n","                amount=conv2d_one_iter_prune_amount,\n","            )\n","        else:\n","            for module_name, module in model.named_modules():\n","                if isinstance(module, torch.nn.Conv2d):\n","                    prune.l1_unstructured(module,\n","                                          name=\"weight\",\n","                                          amount=conv2d_one_iter_prune_amount)\n","                elif isinstance(module, torch.nn.Linear):\n","                    prune.l1_unstructured(module,\n","                                          name=\"weight\",\n","                                          amount=linear_one_iter_prune_amount)\n","\n","        _, eval_accuracy = evaluate_model(model=model,\n","                                          test_loader=test_loader,\n","                                          device=device,\n","                                          criterion=None)\n","\n","        #classification_report = create_classification_report(\n","        #    model=model, test_loader=test_loader, device=device)\n","\n","        num_zeros, num_elements, sparsity = measure_global_sparsity(\n","            model,\n","            weight=True,\n","            bias=False,\n","            conv2d_use_mask=True,\n","            linear_use_mask=False)\n","\n","        print(\"Test Accuracy: {:.3f}\".format(eval_accuracy))\n","        #print(\"Classification Report:\")\n","        #print(classification_report)\n","        print(\"Global Sparsity:\")\n","        print(\"{:.2f}\".format(sparsity))\n","\n","        # print(model.conv1._forward_pre_hooks)\n","        \n","        if (i >= (num_iterations * 2/3)) and (num_iterations >= 3):\n","            cur_num_epochs_per_iter = int(num_epochs_per_iteration * 3/2)\n","        else:\n","            cur_num_epochs_per_iter = num_epochs_per_iteration\n","\n","        print(\"Fine-tuning...\")\n","\n","        train_model(model=model,\n","                    train_loader=train_loader,\n","                    test_loader=test_loader,\n","                    device=device,\n","                    #model_dir=model_dir,\n","                    #model_filename=\"{}_iter{}\".format(model_filename_prefix, i + 1),\n","                    l1_regularization_strength=l1_regularization_strength,\n","                    l2_regularization_strength=l2_regularization_strength,\n","                    weight_decay=weight_decay,\n","                    learning_rate=learning_rate * (learning_rate_decay**i),\n","                    num_epochs=cur_num_epochs_per_iter)\n","        \n","\n","        _, eval_accuracy = evaluate_model(model=model,\n","                                          test_loader=test_loader,\n","                                          device=device,\n","                                          criterion=None)\n","\n","        classification_report = create_classification_report(\n","            model=model, test_loader=test_loader, device=device)\n","\n","        num_zeros, num_elements, sparsity = measure_global_sparsity(\n","            model,\n","            weight=True,\n","            bias=False,\n","            conv2d_use_mask=True,\n","            linear_use_mask=False)\n","\n","        print(\"Test Accuracy: {:.3f}\".format(eval_accuracy))\n","        print(\"Classification Report:\")\n","        print(classification_report)\n","        print(\"Global Sparsity:\")\n","        print(\"{:.2f}\".format(sparsity))\n","\n","        #model_filename = \"{}_{}.pt\".format(model_filename_prefix, i + 1)\n","        #model_filepath = os.path.join(model_dir, model_filename)\n","        \n","        torch.save({\n","                #'epoch': epoch,\n","                'state_dict': model.state_dict(),\n","                #'optimizer': optimizer.state_dict(),\n","            }, f'./checkpoint_iter{i+1}.pth.tar')\n","        \n","        #save_model(model=model,\n","        #           model_dir=model_dir,\n","        #           model_filename=model_filename)\n","        \n","        #model = load_model(model=model,\n","        #                   model_filepath=model_filepath,\n","        #                   device=device)\n","        \n","        torch.cuda.empty_cache()\n","        \n","        \n","    return model\n"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T12:00:55.901260Z","iopub.status.busy":"2024-05-07T12:00:55.900893Z","iopub.status.idle":"2024-05-07T12:00:55.907919Z","shell.execute_reply":"2024-05-07T12:00:55.907018Z","shell.execute_reply.started":"2024-05-07T12:00:55.901232Z"},"trusted":true},"outputs":[],"source":["def remove_parameters(model):\n","\n","    for module_name, module in model.named_modules():\n","        if isinstance(module, torch.nn.Conv2d):\n","            try:\n","                prune.remove(module, \"weight\")\n","            except:\n","                pass\n","            try:\n","                prune.remove(module, \"bias\")\n","            except:\n","                pass\n","        elif isinstance(module, torch.nn.Linear):\n","            try:\n","                prune.remove(module, \"weight\")\n","            except:\n","                pass\n","            try:\n","                prune.remove(module, \"bias\")\n","            except:\n","                pass\n","\n","    return model"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T12:00:59.612610Z","iopub.status.busy":"2024-05-07T12:00:59.611623Z","iopub.status.idle":"2024-05-07T12:00:59.618693Z","shell.execute_reply":"2024-05-07T12:00:59.617752Z","shell.execute_reply.started":"2024-05-07T12:00:59.612573Z"},"trusted":true},"outputs":[],"source":["import torch.nn.utils.prune as prune"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T12:01:00.726710Z","iopub.status.busy":"2024-05-07T12:01:00.725971Z","iopub.status.idle":"2024-05-07T12:01:00.731079Z","shell.execute_reply":"2024-05-07T12:01:00.730094Z","shell.execute_reply.started":"2024-05-07T12:01:00.726676Z"},"trusted":true},"outputs":[],"source":["model_dir = \"saved_models\"\n","model_filename_prefix = \"resnet34_cifar100\"\n","pruned_model_filename = \"resnet34_cifar100_iterative_sp0.95.pt\"\n","pruned_model_filepath = os.path.join(model_dir, pruned_model_filename)"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T12:01:04.988256Z","iopub.status.busy":"2024-05-07T12:01:04.987865Z","iopub.status.idle":"2024-05-07T12:01:04.992640Z","shell.execute_reply":"2024-05-07T12:01:04.991678Z","shell.execute_reply.started":"2024-05-07T12:01:04.988227Z"},"trusted":true},"outputs":[],"source":["model_filepath = \"/kaggle/input/resnet34-for-cifar100/pytorch/accuracy-83.8/1/checkpoint_epoch50.pth.tar\""]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T12:01:07.003859Z","iopub.status.busy":"2024-05-07T12:01:07.003456Z","iopub.status.idle":"2024-05-07T12:01:07.008996Z","shell.execute_reply":"2024-05-07T12:01:07.007901Z","shell.execute_reply.started":"2024-05-07T12:01:07.003833Z"},"trusted":true},"outputs":[],"source":["num_classes = 100\n","random_seed = 17\n","l1_regularization_strength = 0\n","l2_regularization_strength = 0\n","weight_decay = 5e-4\n","learning_rate = 3e-5\n","learning_rate_decay = 1"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T12:01:08.532115Z","iopub.status.busy":"2024-05-07T12:01:08.531738Z","iopub.status.idle":"2024-05-07T12:01:08.546756Z","shell.execute_reply":"2024-05-07T12:01:08.545755Z","shell.execute_reply.started":"2024-05-07T12:01:08.532087Z"},"trusted":true},"outputs":[],"source":["mean = torch.tensor([0.5070, 0.4865, 0.4408]) # CIFAR100 train mean\n","std = torch.tensor([0.2621, 0.2512, 0.2713]) # CIFAR100 train std"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T12:01:13.486232Z","iopub.status.busy":"2024-05-07T12:01:13.485254Z","iopub.status.idle":"2024-05-07T12:01:13.515439Z","shell.execute_reply":"2024-05-07T12:01:13.514233Z","shell.execute_reply.started":"2024-05-07T12:01:13.486188Z"},"trusted":true},"outputs":[{"data":{"text/plain":["True"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["torch.cuda.is_available()"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T12:01:14.980559Z","iopub.status.busy":"2024-05-07T12:01:14.979933Z","iopub.status.idle":"2024-05-07T12:01:14.985104Z","shell.execute_reply":"2024-05-07T12:01:14.983946Z","shell.execute_reply.started":"2024-05-07T12:01:14.980527Z"},"trusted":true},"outputs":[],"source":["cuda_device = torch.device(\"cuda:0\")\n","cpu_device = torch.device(\"cpu:0\")"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T12:01:16.460985Z","iopub.status.busy":"2024-05-07T12:01:16.460367Z","iopub.status.idle":"2024-05-07T12:01:16.467114Z","shell.execute_reply":"2024-05-07T12:01:16.466185Z","shell.execute_reply.started":"2024-05-07T12:01:16.460946Z"},"trusted":true},"outputs":[],"source":["set_random_seeds(random_seed=random_seed)"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T12:01:19.165701Z","iopub.status.busy":"2024-05-07T12:01:19.164819Z","iopub.status.idle":"2024-05-07T12:01:23.503644Z","shell.execute_reply":"2024-05-07T12:01:23.502609Z","shell.execute_reply.started":"2024-05-07T12:01:19.165666Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n","  warnings.warn(msg)\n"]}],"source":["model = create_model(num_classes=num_classes)\n","\n","    # Load a pretrained model.\n","model = load_model(model=model,\n","                    model_filepath=model_filepath,\n","                    device=cuda_device) # cuda_device!!!"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T12:01:40.946770Z","iopub.status.busy":"2024-05-07T12:01:40.945813Z","iopub.status.idle":"2024-05-07T12:01:48.256892Z","shell.execute_reply":"2024-05-07T12:01:48.255834Z","shell.execute_reply.started":"2024-05-07T12:01:40.946734Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_34/2226863683.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  torchvision.transforms.Normalize(mean=torch.tensor(mean), std=stdev)\n"]},{"name":"stdout","output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to data/cifar-100-python.tar.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 169001437/169001437 [00:03<00:00, 47257682.42it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting data/cifar-100-python.tar.gz to data\n","Files already downloaded and verified\n"]}],"source":["train_loader, test_loader, classes = prepare_dataloader(\n","        num_workers=2, train_batch_size=128, eval_batch_size=128, mean=mean, stdev=std)"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T12:01:54.098495Z","iopub.status.busy":"2024-05-07T12:01:54.097796Z","iopub.status.idle":"2024-05-07T12:02:05.553620Z","shell.execute_reply":"2024-05-07T12:02:05.552460Z","shell.execute_reply.started":"2024-05-07T12:01:54.098462Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(0.8382, device='cuda:0')\n"]}],"source":["\n","_, eval_accuracy = evaluate_model(model=model,\n","                                    test_loader=test_loader,\n","                                    device=cuda_device,\n","                                    criterion=None)\n","print(eval_accuracy)"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T07:34:23.831393Z","iopub.status.busy":"2024-05-07T07:34:23.831038Z","iopub.status.idle":"2024-05-07T07:34:23.835883Z","shell.execute_reply":"2024-05-07T07:34:23.834879Z","shell.execute_reply.started":"2024-05-07T07:34:23.831360Z"},"trusted":true},"outputs":[],"source":["\n","#classification_report = create_classification_report(\n","#        model=model, test_loader=test_loader, device=cuda_device)\n"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T08:53:39.291227Z","iopub.status.busy":"2024-05-07T08:53:39.290914Z","iopub.status.idle":"2024-05-07T08:53:39.301994Z","shell.execute_reply":"2024-05-07T08:53:39.301068Z","shell.execute_reply.started":"2024-05-07T08:53:39.291200Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Accuracy: 0.838\n","Global Sparsity:\n","0.00\n"]}],"source":["\n","num_zeros, num_elements, sparsity = measure_global_sparsity(model)\n","\n","print(\"Test Accuracy: {:.3f}\".format(eval_accuracy))\n","#print(\"Classification Report:\")\n","#print(classification_report)\n","print(\"Global Sparsity:\")\n","print(\"{:.2f}\".format(sparsity))\n"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T12:02:25.384913Z","iopub.status.busy":"2024-05-07T12:02:25.384128Z","iopub.status.idle":"2024-05-07T12:02:25.421345Z","shell.execute_reply":"2024-05-07T12:02:25.420543Z","shell.execute_reply.started":"2024-05-07T12:02:25.384876Z"},"trusted":true},"outputs":[],"source":["pruned_model = copy.deepcopy(model)"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T12:02:26.746471Z","iopub.status.busy":"2024-05-07T12:02:26.745741Z","iopub.status.idle":"2024-05-07T12:02:26.756441Z","shell.execute_reply":"2024-05-07T12:02:26.755489Z","shell.execute_reply.started":"2024-05-07T12:02:26.746440Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0.0\n"]}],"source":["num_zeros, num_elements, sparsity = measure_global_sparsity(pruned_model)\n","print(sparsity)"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T12:02:32.767646Z","iopub.status.busy":"2024-05-07T12:02:32.767041Z","iopub.status.idle":"2024-05-07T12:02:33.776366Z","shell.execute_reply":"2024-05-07T12:02:33.775280Z","shell.execute_reply.started":"2024-05-07T12:02:32.767612Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Tue May  7 12:02:33 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla P100-PCIE-16GB           Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   44C    P0              33W / 250W |   8914MiB / 16384MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T12:02:41.864855Z","iopub.status.busy":"2024-05-07T12:02:41.864243Z","iopub.status.idle":"2024-05-07T12:02:41.869101Z","shell.execute_reply":"2024-05-07T12:02:41.868017Z","shell.execute_reply.started":"2024-05-07T12:02:41.864826Z"},"trusted":true},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T12:02:38.310895Z","iopub.status.busy":"2024-05-07T12:02:38.310508Z","iopub.status.idle":"2024-05-07T12:02:38.315308Z","shell.execute_reply":"2024-05-07T12:02:38.314329Z","shell.execute_reply.started":"2024-05-07T12:02:38.310863Z"},"trusted":true},"outputs":[],"source":["import gc"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T12:02:46.498556Z","iopub.status.busy":"2024-05-07T12:02:46.497852Z","iopub.status.idle":"2024-05-07T12:02:46.660335Z","shell.execute_reply":"2024-05-07T12:02:46.659367Z","shell.execute_reply.started":"2024-05-07T12:02:46.498522Z"},"trusted":true},"outputs":[{"data":{"text/plain":["0"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["gc.collect()"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T12:02:49.459431Z","iopub.status.busy":"2024-05-07T12:02:49.458627Z","iopub.status.idle":"2024-05-07T12:02:50.467715Z","shell.execute_reply":"2024-05-07T12:02:50.466672Z","shell.execute_reply.started":"2024-05-07T12:02:49.459398Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Tue May  7 12:02:50 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla P100-PCIE-16GB           Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   44C    P0              38W / 250W |    500MiB / 16384MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T12:02:53.899327Z","iopub.status.busy":"2024-05-07T12:02:53.898300Z","iopub.status.idle":"2024-05-07T12:02:53.906011Z","shell.execute_reply":"2024-05-07T12:02:53.905017Z","shell.execute_reply.started":"2024-05-07T12:02:53.899289Z"},"trusted":true},"outputs":[{"data":{"text/plain":["True"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["next(pruned_model.parameters()).is_cuda"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T08:33:09.121090Z","iopub.status.busy":"2024-05-07T08:33:09.120248Z","iopub.status.idle":"2024-05-07T08:33:09.154812Z","shell.execute_reply":"2024-05-07T08:33:09.153917Z","shell.execute_reply.started":"2024-05-07T08:33:09.121058Z"},"trusted":true},"outputs":[],"source":["pruned_model.to(cuda_device);"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T08:33:09.683930Z","iopub.status.busy":"2024-05-07T08:33:09.683157Z","iopub.status.idle":"2024-05-07T08:33:09.689408Z","shell.execute_reply":"2024-05-07T08:33:09.688492Z","shell.execute_reply.started":"2024-05-07T08:33:09.683901Z"},"trusted":true},"outputs":[{"data":{"text/plain":["True"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["next(pruned_model.parameters()).is_cuda"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T08:33:10.563505Z","iopub.status.busy":"2024-05-07T08:33:10.562719Z","iopub.status.idle":"2024-05-07T08:33:11.577755Z","shell.execute_reply":"2024-05-07T08:33:11.576632Z","shell.execute_reply.started":"2024-05-07T08:33:10.563478Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Tue May  7 08:33:11 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla P100-PCIE-16GB           Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   36C    P0              32W / 250W |    536MiB / 16384MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-29T12:24:20.174310Z","iopub.status.idle":"2024-03-29T12:24:20.174707Z","shell.execute_reply":"2024-03-29T12:24:20.174542Z","shell.execute_reply.started":"2024-03-29T12:24:20.174526Z"},"trusted":true},"outputs":[],"source":["# Epoch: 001 Train Loss: 0.910 Train Acc: 0.734 Eval Loss: 0.865 Eval Acc: 0.750,   random_seed=21"]},{"cell_type":"code","execution_count":38,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-05-07T12:03:08.771008Z","iopub.status.busy":"2024-05-07T12:03:08.770625Z","iopub.status.idle":"2024-05-07T14:54:22.897456Z","shell.execute_reply":"2024-05-07T14:54:22.896244Z","shell.execute_reply.started":"2024-05-07T12:03:08.770978Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Pruning...\n","Pruning and Finetuning 1/6\n","Pruning...\n","Test Accuracy: 0.827\n","Global Sparsity:\n","0.39\n","Fine-tuning...\n","Epoch: 000 Eval Loss: 0.608 Eval Acc: 0.827\n","Epoch: 001 Train Loss: 0.285 Train Acc: 0.926 Eval Loss: 0.610 Eval Acc: 0.828\n","Epoch time: 143.7 Total training time: 155.2\n","\n","Epoch: 002 Train Loss: 0.275 Train Acc: 0.928 Eval Loss: 0.609 Eval Acc: 0.826\n","Epoch time: 142.9 Total training time: 298.2\n","\n","Epoch: 003 Train Loss: 0.259 Train Acc: 0.932 Eval Loss: 0.607 Eval Acc: 0.829\n","Epoch time: 143.1 Total training time: 441.2\n","\n","Epoch: 004 Train Loss: 0.250 Train Acc: 0.935 Eval Loss: 0.605 Eval Acc: 0.831\n","Epoch time: 143.1 Total training time: 584.3\n","\n","Epoch: 005 Train Loss: 0.238 Train Acc: 0.938 Eval Loss: 0.605 Eval Acc: 0.834\n","Epoch time: 143.4 Total training time: 727.7\n","\n","Epoch: 006 Train Loss: 0.233 Train Acc: 0.940 Eval Loss: 0.609 Eval Acc: 0.832\n","Epoch time: 143.2 Total training time: 870.9\n","\n","Epoch: 007 Train Loss: 0.233 Train Acc: 0.940 Eval Loss: 0.611 Eval Acc: 0.833\n","Epoch time: 143.4 Total training time: 1014.3\n","\n","Epoch: 008 Train Loss: 0.226 Train Acc: 0.941 Eval Loss: 0.610 Eval Acc: 0.833\n","Epoch time: 142.9 Total training time: 1157.2\n","\n","Epoch: 009 Train Loss: 0.215 Train Acc: 0.945 Eval Loss: 0.607 Eval Acc: 0.831\n","Epoch time: 143.0 Total training time: 1300.1\n","\n","Epoch: 010 Train Loss: 0.208 Train Acc: 0.946 Eval Loss: 0.609 Eval Acc: 0.830\n","Epoch time: 143.5 Total training time: 1443.6\n","\n","Test Accuracy: 0.830\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.93      0.94      0.94       100\n","           1       0.90      0.94      0.92       100\n","           2       0.70      0.69      0.69       100\n","           3       0.78      0.82      0.80       100\n","           4       0.66      0.75      0.70       100\n","           5       0.86      0.85      0.85       100\n","           6       0.85      0.88      0.86       100\n","           7       0.91      0.79      0.84       100\n","           8       0.98      0.93      0.95       100\n","           9       0.96      0.93      0.94       100\n","          10       0.74      0.67      0.70       100\n","          11       0.56      0.54      0.55       100\n","          12       0.89      0.90      0.90       100\n","          13       0.88      0.74      0.80       100\n","          14       0.88      0.91      0.89       100\n","          15       0.87      0.90      0.88       100\n","          16       0.88      0.86      0.87       100\n","          17       0.88      0.92      0.90       100\n","          18       0.88      0.83      0.86       100\n","          19       0.78      0.83      0.81       100\n","          20       0.92      0.89      0.90       100\n","          21       0.90      0.92      0.91       100\n","          22       0.83      0.91      0.87       100\n","          23       0.84      0.91      0.87       100\n","          24       0.91      0.88      0.89       100\n","          25       0.75      0.81      0.78       100\n","          26       0.72      0.79      0.75       100\n","          27       0.84      0.80      0.82       100\n","          28       0.90      0.89      0.89       100\n","          29       0.90      0.78      0.83       100\n","          30       0.78      0.78      0.78       100\n","          31       0.97      0.75      0.85       100\n","          32       0.85      0.79      0.82       100\n","          33       0.82      0.75      0.79       100\n","          34       0.86      0.92      0.89       100\n","          35       0.59      0.66      0.62       100\n","          36       0.84      0.89      0.86       100\n","          37       0.88      0.81      0.84       100\n","          38       0.81      0.88      0.85       100\n","          39       0.97      0.92      0.94       100\n","          40       0.92      0.86      0.89       100\n","          41       0.96      0.94      0.95       100\n","          42       0.85      0.80      0.82       100\n","          43       0.89      0.91      0.90       100\n","          44       0.72      0.81      0.76       100\n","          45       0.76      0.71      0.74       100\n","          46       0.67      0.67      0.67       100\n","          47       0.60      0.68      0.64       100\n","          48       0.87      0.99      0.93       100\n","          49       0.88      0.89      0.89       100\n","          50       0.73      0.64      0.68       100\n","          51       0.90      0.89      0.89       100\n","          52       0.68      0.61      0.64       100\n","          53       0.91      0.96      0.93       100\n","          54       0.84      0.81      0.83       100\n","          55       0.68      0.69      0.69       100\n","          56       0.94      0.93      0.93       100\n","          57       0.86      0.84      0.85       100\n","          58       0.92      0.98      0.95       100\n","          59       0.73      0.67      0.70       100\n","          60       0.85      0.87      0.86       100\n","          61       0.87      0.76      0.81       100\n","          62       0.88      0.78      0.83       100\n","          63       0.79      0.80      0.80       100\n","          64       0.76      0.71      0.74       100\n","          65       0.84      0.76      0.80       100\n","          66       0.96      0.88      0.92       100\n","          67       0.80      0.73      0.76       100\n","          68       0.90      0.93      0.92       100\n","          69       0.86      0.92      0.89       100\n","          70       0.82      0.86      0.84       100\n","          71       0.88      0.78      0.83       100\n","          72       0.56      0.70      0.62       100\n","          73       0.82      0.69      0.75       100\n","          74       0.69      0.65      0.67       100\n","          75       0.90      0.93      0.92       100\n","          76       0.92      0.95      0.94       100\n","          77       0.87      0.91      0.89       100\n","          78       0.74      0.76      0.75       100\n","          79       0.76      0.94      0.84       100\n","          80       0.84      0.84      0.84       100\n","          81       0.76      0.83      0.79       100\n","          82       0.93      0.97      0.95       100\n","          83       0.81      0.85      0.83       100\n","          84       0.94      0.85      0.89       100\n","          85       0.91      0.90      0.90       100\n","          86       0.93      0.89      0.91       100\n","          87       0.89      0.93      0.91       100\n","          88       0.80      0.93      0.86       100\n","          89       0.91      0.96      0.94       100\n","          90       0.83      0.86      0.84       100\n","          91       0.91      0.89      0.90       100\n","          92       0.70      0.81      0.75       100\n","          93       0.87      0.76      0.81       100\n","          94       0.93      0.96      0.95       100\n","          95       0.80      0.81      0.81       100\n","          96       0.66      0.67      0.67       100\n","          97       0.91      0.90      0.90       100\n","          98       0.67      0.68      0.67       100\n","          99       0.83      0.79      0.81       100\n","\n","    accuracy                           0.83     10000\n","   macro avg       0.83      0.83      0.83     10000\n","weighted avg       0.83      0.83      0.83     10000\n","\n","Global Sparsity:\n","0.39\n","Pruning and Finetuning 2/6\n","Pruning...\n","Test Accuracy: 0.742\n","Global Sparsity:\n","0.63\n","Fine-tuning...\n","Epoch: 000 Eval Loss: 0.970 Eval Acc: 0.742\n","Epoch: 001 Train Loss: 0.254 Train Acc: 0.936 Eval Loss: 0.615 Eval Acc: 0.828\n","Epoch time: 142.9 Total training time: 154.0\n","\n","Epoch: 002 Train Loss: 0.214 Train Acc: 0.946 Eval Loss: 0.611 Eval Acc: 0.831\n","Epoch time: 143.3 Total training time: 297.3\n","\n","Epoch: 003 Train Loss: 0.199 Train Acc: 0.950 Eval Loss: 0.615 Eval Acc: 0.828\n","Epoch time: 143.0 Total training time: 440.3\n","\n","Epoch: 004 Train Loss: 0.191 Train Acc: 0.952 Eval Loss: 0.620 Eval Acc: 0.831\n","Epoch time: 143.4 Total training time: 583.7\n","\n","Epoch: 005 Train Loss: 0.183 Train Acc: 0.954 Eval Loss: 0.609 Eval Acc: 0.834\n","Epoch time: 143.0 Total training time: 726.7\n","\n","Epoch: 006 Train Loss: 0.182 Train Acc: 0.953 Eval Loss: 0.608 Eval Acc: 0.837\n","Epoch time: 143.3 Total training time: 870.0\n","\n","Epoch: 007 Train Loss: 0.180 Train Acc: 0.955 Eval Loss: 0.614 Eval Acc: 0.834\n","Epoch time: 143.0 Total training time: 1013.0\n","\n","Epoch: 008 Train Loss: 0.174 Train Acc: 0.956 Eval Loss: 0.609 Eval Acc: 0.833\n","Epoch time: 143.3 Total training time: 1156.3\n","\n","Epoch: 009 Train Loss: 0.167 Train Acc: 0.956 Eval Loss: 0.607 Eval Acc: 0.835\n","Epoch time: 142.9 Total training time: 1299.2\n","\n","Epoch: 010 Train Loss: 0.168 Train Acc: 0.957 Eval Loss: 0.604 Eval Acc: 0.838\n","Epoch time: 144.3 Total training time: 1443.5\n","\n","Test Accuracy: 0.838\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.93      0.95      0.94       100\n","           1       0.92      0.91      0.91       100\n","           2       0.74      0.72      0.73       100\n","           3       0.73      0.82      0.77       100\n","           4       0.69      0.75      0.72       100\n","           5       0.79      0.93      0.86       100\n","           6       0.89      0.87      0.88       100\n","           7       0.83      0.85      0.84       100\n","           8       0.95      0.93      0.94       100\n","           9       0.95      0.92      0.93       100\n","          10       0.77      0.59      0.67       100\n","          11       0.58      0.73      0.65       100\n","          12       0.83      0.91      0.87       100\n","          13       0.86      0.76      0.81       100\n","          14       0.90      0.88      0.89       100\n","          15       0.90      0.93      0.92       100\n","          16       0.92      0.87      0.89       100\n","          17       0.91      0.90      0.90       100\n","          18       0.87      0.84      0.85       100\n","          19       0.91      0.82      0.86       100\n","          20       0.92      0.88      0.90       100\n","          21       0.87      0.95      0.91       100\n","          22       0.88      0.93      0.90       100\n","          23       0.89      0.85      0.87       100\n","          24       0.90      0.93      0.92       100\n","          25       0.83      0.73      0.78       100\n","          26       0.82      0.84      0.83       100\n","          27       0.79      0.84      0.81       100\n","          28       0.91      0.91      0.91       100\n","          29       0.76      0.83      0.79       100\n","          30       0.82      0.76      0.79       100\n","          31       0.84      0.84      0.84       100\n","          32       0.81      0.78      0.80       100\n","          33       0.79      0.79      0.79       100\n","          34       0.92      0.90      0.91       100\n","          35       0.69      0.66      0.68       100\n","          36       0.85      0.91      0.88       100\n","          37       0.85      0.83      0.84       100\n","          38       0.86      0.85      0.85       100\n","          39       0.97      0.91      0.94       100\n","          40       0.91      0.83      0.87       100\n","          41       0.92      0.94      0.93       100\n","          42       0.86      0.80      0.83       100\n","          43       0.86      0.94      0.90       100\n","          44       0.82      0.75      0.79       100\n","          45       0.76      0.77      0.77       100\n","          46       0.74      0.67      0.70       100\n","          47       0.64      0.66      0.65       100\n","          48       0.93      0.97      0.95       100\n","          49       0.87      0.88      0.88       100\n","          50       0.66      0.61      0.63       100\n","          51       0.86      0.87      0.87       100\n","          52       0.69      0.64      0.66       100\n","          53       0.91      0.96      0.94       100\n","          54       0.85      0.85      0.85       100\n","          55       0.73      0.65      0.69       100\n","          56       0.90      0.94      0.92       100\n","          57       0.78      0.87      0.82       100\n","          58       0.92      0.99      0.95       100\n","          59       0.74      0.69      0.72       100\n","          60       0.88      0.84      0.86       100\n","          61       0.81      0.86      0.83       100\n","          62       0.74      0.87      0.80       100\n","          63       0.80      0.85      0.83       100\n","          64       0.76      0.73      0.74       100\n","          65       0.87      0.78      0.82       100\n","          66       0.91      0.90      0.90       100\n","          67       0.83      0.74      0.78       100\n","          68       0.89      0.98      0.93       100\n","          69       0.87      0.93      0.90       100\n","          70       0.84      0.80      0.82       100\n","          71       0.81      0.87      0.84       100\n","          72       0.69      0.67      0.68       100\n","          73       0.72      0.80      0.76       100\n","          74       0.67      0.64      0.65       100\n","          75       0.91      0.93      0.92       100\n","          76       0.95      0.94      0.94       100\n","          77       0.90      0.90      0.90       100\n","          78       0.80      0.78      0.79       100\n","          79       0.88      0.89      0.89       100\n","          80       0.76      0.86      0.81       100\n","          81       0.79      0.86      0.82       100\n","          82       0.98      0.94      0.96       100\n","          83       0.88      0.81      0.84       100\n","          84       0.90      0.88      0.89       100\n","          85       0.95      0.90      0.92       100\n","          86       0.97      0.88      0.92       100\n","          87       0.88      0.93      0.90       100\n","          88       0.86      0.88      0.87       100\n","          89       0.88      0.98      0.92       100\n","          90       0.85      0.88      0.86       100\n","          91       0.91      0.91      0.91       100\n","          92       0.82      0.71      0.76       100\n","          93       0.85      0.86      0.86       100\n","          94       0.92      0.96      0.94       100\n","          95       0.90      0.75      0.82       100\n","          96       0.70      0.63      0.66       100\n","          97       0.92      0.90      0.91       100\n","          98       0.66      0.68      0.67       100\n","          99       0.85      0.84      0.84       100\n","\n","    accuracy                           0.84     10000\n","   macro avg       0.84      0.84      0.84     10000\n","weighted avg       0.84      0.84      0.84     10000\n","\n","Global Sparsity:\n","0.63\n","Pruning and Finetuning 3/6\n","Pruning...\n","Test Accuracy: 0.559\n","Global Sparsity:\n","0.77\n","Fine-tuning...\n","Epoch: 000 Eval Loss: 1.795 Eval Acc: 0.559\n","Epoch: 001 Train Loss: 0.344 Train Acc: 0.911 Eval Loss: 0.634 Eval Acc: 0.823\n","Epoch time: 143.1 Total training time: 154.1\n","\n","Epoch: 002 Train Loss: 0.246 Train Acc: 0.939 Eval Loss: 0.619 Eval Acc: 0.829\n","Epoch time: 143.1 Total training time: 297.2\n","\n","Epoch: 003 Train Loss: 0.217 Train Acc: 0.947 Eval Loss: 0.608 Eval Acc: 0.829\n","Epoch time: 143.1 Total training time: 440.3\n","\n","Epoch: 004 Train Loss: 0.205 Train Acc: 0.949 Eval Loss: 0.600 Eval Acc: 0.834\n","Epoch time: 143.0 Total training time: 583.4\n","\n","Epoch: 005 Train Loss: 0.195 Train Acc: 0.952 Eval Loss: 0.605 Eval Acc: 0.832\n","Epoch time: 143.4 Total training time: 726.8\n","\n","Epoch: 006 Train Loss: 0.185 Train Acc: 0.954 Eval Loss: 0.600 Eval Acc: 0.833\n","Epoch time: 143.2 Total training time: 870.0\n","\n","Epoch: 007 Train Loss: 0.173 Train Acc: 0.957 Eval Loss: 0.599 Eval Acc: 0.836\n","Epoch time: 143.2 Total training time: 1013.1\n","\n","Epoch: 008 Train Loss: 0.176 Train Acc: 0.956 Eval Loss: 0.607 Eval Acc: 0.837\n","Epoch time: 142.9 Total training time: 1156.0\n","\n","Epoch: 009 Train Loss: 0.173 Train Acc: 0.958 Eval Loss: 0.607 Eval Acc: 0.832\n","Epoch time: 143.2 Total training time: 1299.3\n","\n","Epoch: 010 Train Loss: 0.164 Train Acc: 0.960 Eval Loss: 0.605 Eval Acc: 0.835\n","Epoch time: 144.0 Total training time: 1443.3\n","\n","Test Accuracy: 0.835\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.89      0.94      0.91       100\n","           1       0.94      0.91      0.92       100\n","           2       0.74      0.78      0.76       100\n","           3       0.74      0.81      0.77       100\n","           4       0.74      0.75      0.74       100\n","           5       0.82      0.88      0.85       100\n","           6       0.80      0.89      0.84       100\n","           7       0.84      0.81      0.83       100\n","           8       0.95      0.92      0.93       100\n","           9       0.95      0.93      0.94       100\n","          10       0.76      0.66      0.71       100\n","          11       0.60      0.64      0.62       100\n","          12       0.87      0.90      0.88       100\n","          13       0.86      0.77      0.81       100\n","          14       0.90      0.91      0.91       100\n","          15       0.87      0.90      0.89       100\n","          16       0.89      0.86      0.87       100\n","          17       0.91      0.88      0.89       100\n","          18       0.87      0.80      0.83       100\n","          19       0.82      0.87      0.84       100\n","          20       0.88      0.90      0.89       100\n","          21       0.90      0.95      0.93       100\n","          22       0.93      0.87      0.90       100\n","          23       0.91      0.86      0.89       100\n","          24       0.93      0.86      0.90       100\n","          25       0.78      0.73      0.76       100\n","          26       0.79      0.81      0.80       100\n","          27       0.82      0.81      0.81       100\n","          28       0.91      0.89      0.90       100\n","          29       0.86      0.84      0.85       100\n","          30       0.77      0.81      0.79       100\n","          31       0.83      0.86      0.84       100\n","          32       0.78      0.78      0.78       100\n","          33       0.79      0.73      0.76       100\n","          34       0.87      0.92      0.89       100\n","          35       0.63      0.63      0.63       100\n","          36       0.82      0.88      0.85       100\n","          37       0.90      0.80      0.85       100\n","          38       0.85      0.85      0.85       100\n","          39       0.98      0.94      0.96       100\n","          40       0.90      0.84      0.87       100\n","          41       0.95      0.94      0.94       100\n","          42       0.72      0.86      0.79       100\n","          43       0.89      0.93      0.91       100\n","          44       0.75      0.84      0.79       100\n","          45       0.82      0.72      0.77       100\n","          46       0.66      0.70      0.68       100\n","          47       0.64      0.67      0.66       100\n","          48       0.92      0.96      0.94       100\n","          49       0.86      0.90      0.88       100\n","          50       0.70      0.68      0.69       100\n","          51       0.85      0.90      0.87       100\n","          52       0.61      0.70      0.65       100\n","          53       0.92      0.98      0.95       100\n","          54       0.80      0.90      0.85       100\n","          55       0.70      0.64      0.67       100\n","          56       0.91      0.96      0.93       100\n","          57       0.85      0.86      0.86       100\n","          58       0.97      0.97      0.97       100\n","          59       0.79      0.56      0.65       100\n","          60       0.88      0.84      0.86       100\n","          61       0.82      0.82      0.82       100\n","          62       0.83      0.80      0.82       100\n","          63       0.72      0.86      0.79       100\n","          64       0.81      0.72      0.76       100\n","          65       0.84      0.76      0.80       100\n","          66       0.93      0.89      0.91       100\n","          67       0.79      0.77      0.78       100\n","          68       0.87      0.97      0.92       100\n","          69       0.86      0.91      0.88       100\n","          70       0.86      0.84      0.85       100\n","          71       0.83      0.86      0.85       100\n","          72       0.69      0.73      0.71       100\n","          73       0.79      0.76      0.78       100\n","          74       0.72      0.57      0.64       100\n","          75       0.97      0.89      0.93       100\n","          76       0.90      0.95      0.92       100\n","          77       0.92      0.85      0.89       100\n","          78       0.83      0.75      0.79       100\n","          79       0.78      0.94      0.85       100\n","          80       0.78      0.83      0.80       100\n","          81       0.80      0.84      0.82       100\n","          82       0.96      0.95      0.95       100\n","          83       0.83      0.81      0.82       100\n","          84       0.87      0.85      0.86       100\n","          85       0.91      0.93      0.92       100\n","          86       0.98      0.89      0.93       100\n","          87       0.91      0.91      0.91       100\n","          88       0.89      0.85      0.87       100\n","          89       0.92      0.97      0.94       100\n","          90       0.85      0.86      0.86       100\n","          91       0.92      0.90      0.91       100\n","          92       0.77      0.76      0.76       100\n","          93       0.88      0.84      0.86       100\n","          94       0.92      0.97      0.95       100\n","          95       0.84      0.79      0.81       100\n","          96       0.66      0.67      0.66       100\n","          97       0.92      0.86      0.89       100\n","          98       0.66      0.59      0.62       100\n","          99       0.83      0.89      0.86       100\n","\n","    accuracy                           0.83     10000\n","   macro avg       0.84      0.83      0.83     10000\n","weighted avg       0.84      0.83      0.83     10000\n","\n","Global Sparsity:\n","0.77\n","Pruning and Finetuning 4/6\n","Pruning...\n","Test Accuracy: 0.278\n","Global Sparsity:\n","0.86\n","Fine-tuning...\n","Epoch: 000 Eval Loss: 3.489 Eval Acc: 0.278\n","Epoch: 001 Train Loss: 0.593 Train Acc: 0.846 Eval Loss: 0.719 Eval Acc: 0.799\n","Epoch time: 142.9 Total training time: 154.1\n","\n","Epoch: 002 Train Loss: 0.368 Train Acc: 0.906 Eval Loss: 0.669 Eval Acc: 0.814\n","Epoch time: 143.0 Total training time: 297.1\n","\n","Epoch: 003 Train Loss: 0.300 Train Acc: 0.926 Eval Loss: 0.647 Eval Acc: 0.822\n","Epoch time: 142.8 Total training time: 439.9\n","\n","Epoch: 004 Train Loss: 0.272 Train Acc: 0.933 Eval Loss: 0.639 Eval Acc: 0.821\n","Epoch time: 142.7 Total training time: 582.6\n","\n","Epoch: 005 Train Loss: 0.248 Train Acc: 0.939 Eval Loss: 0.636 Eval Acc: 0.825\n","Epoch time: 143.1 Total training time: 725.7\n","\n","Epoch: 006 Train Loss: 0.222 Train Acc: 0.946 Eval Loss: 0.627 Eval Acc: 0.827\n","Epoch time: 142.9 Total training time: 868.6\n","\n","Epoch: 007 Train Loss: 0.215 Train Acc: 0.948 Eval Loss: 0.629 Eval Acc: 0.828\n","Epoch time: 142.8 Total training time: 1011.3\n","\n","Epoch: 008 Train Loss: 0.203 Train Acc: 0.951 Eval Loss: 0.625 Eval Acc: 0.829\n","Epoch time: 142.7 Total training time: 1154.0\n","\n","Epoch: 009 Train Loss: 0.196 Train Acc: 0.952 Eval Loss: 0.625 Eval Acc: 0.828\n","Epoch time: 142.5 Total training time: 1296.5\n","\n","Epoch: 010 Train Loss: 0.194 Train Acc: 0.953 Eval Loss: 0.624 Eval Acc: 0.831\n","Epoch time: 143.6 Total training time: 1440.1\n","\n","Test Accuracy: 0.831\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.97      0.95      0.96       100\n","           1       0.94      0.92      0.93       100\n","           2       0.77      0.72      0.74       100\n","           3       0.77      0.81      0.79       100\n","           4       0.76      0.73      0.74       100\n","           5       0.83      0.91      0.87       100\n","           6       0.90      0.85      0.88       100\n","           7       0.83      0.81      0.82       100\n","           8       0.93      0.96      0.95       100\n","           9       0.89      0.92      0.91       100\n","          10       0.81      0.58      0.67       100\n","          11       0.51      0.55      0.53       100\n","          12       0.85      0.88      0.87       100\n","          13       0.78      0.78      0.78       100\n","          14       0.91      0.87      0.89       100\n","          15       0.88      0.90      0.89       100\n","          16       0.84      0.87      0.86       100\n","          17       0.95      0.88      0.91       100\n","          18       0.87      0.82      0.85       100\n","          19       0.91      0.86      0.89       100\n","          20       0.93      0.88      0.90       100\n","          21       0.93      0.91      0.92       100\n","          22       0.88      0.89      0.89       100\n","          23       0.88      0.82      0.85       100\n","          24       0.90      0.87      0.88       100\n","          25       0.71      0.77      0.74       100\n","          26       0.81      0.83      0.82       100\n","          27       0.78      0.83      0.81       100\n","          28       0.84      0.87      0.86       100\n","          29       0.87      0.91      0.89       100\n","          30       0.75      0.83      0.79       100\n","          31       0.90      0.82      0.86       100\n","          32       0.81      0.81      0.81       100\n","          33       0.79      0.72      0.75       100\n","          34       0.81      0.92      0.86       100\n","          35       0.57      0.67      0.61       100\n","          36       0.81      0.92      0.86       100\n","          37       0.92      0.83      0.87       100\n","          38       0.82      0.87      0.84       100\n","          39       0.95      0.93      0.94       100\n","          40       0.87      0.81      0.84       100\n","          41       0.93      0.95      0.94       100\n","          42       0.84      0.82      0.83       100\n","          43       0.88      0.93      0.90       100\n","          44       0.79      0.77      0.78       100\n","          45       0.77      0.77      0.77       100\n","          46       0.67      0.56      0.61       100\n","          47       0.69      0.62      0.65       100\n","          48       0.89      0.98      0.93       100\n","          49       0.82      0.90      0.86       100\n","          50       0.73      0.66      0.69       100\n","          51       0.83      0.90      0.87       100\n","          52       0.65      0.73      0.69       100\n","          53       0.92      0.94      0.93       100\n","          54       0.87      0.90      0.89       100\n","          55       0.62      0.69      0.65       100\n","          56       0.90      0.94      0.92       100\n","          57       0.84      0.88      0.86       100\n","          58       0.91      0.96      0.94       100\n","          59       0.78      0.68      0.73       100\n","          60       0.89      0.86      0.87       100\n","          61       0.79      0.85      0.82       100\n","          62       0.82      0.80      0.81       100\n","          63       0.80      0.80      0.80       100\n","          64       0.76      0.69      0.72       100\n","          65       0.79      0.84      0.81       100\n","          66       0.93      0.88      0.90       100\n","          67       0.80      0.69      0.74       100\n","          68       0.88      0.97      0.92       100\n","          69       0.88      0.91      0.89       100\n","          70       0.81      0.82      0.82       100\n","          71       0.78      0.83      0.80       100\n","          72       0.72      0.64      0.68       100\n","          73       0.73      0.81      0.77       100\n","          74       0.74      0.63      0.68       100\n","          75       0.93      0.94      0.94       100\n","          76       0.89      0.92      0.91       100\n","          77       0.89      0.87      0.88       100\n","          78       0.76      0.81      0.79       100\n","          79       0.86      0.90      0.88       100\n","          80       0.80      0.84      0.82       100\n","          81       0.83      0.78      0.80       100\n","          82       0.94      0.95      0.95       100\n","          83       0.80      0.79      0.79       100\n","          84       0.90      0.83      0.86       100\n","          85       0.95      0.88      0.91       100\n","          86       0.90      0.91      0.91       100\n","          87       0.89      0.93      0.91       100\n","          88       0.89      0.86      0.87       100\n","          89       0.89      0.92      0.91       100\n","          90       0.85      0.86      0.86       100\n","          91       0.90      0.87      0.88       100\n","          92       0.74      0.73      0.74       100\n","          93       0.92      0.83      0.87       100\n","          94       0.95      0.96      0.96       100\n","          95       0.86      0.78      0.82       100\n","          96       0.64      0.70      0.67       100\n","          97       0.88      0.88      0.88       100\n","          98       0.65      0.67      0.66       100\n","          99       0.87      0.84      0.85       100\n","\n","    accuracy                           0.83     10000\n","   macro avg       0.83      0.83      0.83     10000\n","weighted avg       0.83      0.83      0.83     10000\n","\n","Global Sparsity:\n","0.86\n","Pruning and Finetuning 5/6\n","Pruning...\n","Test Accuracy: 0.130\n","Global Sparsity:\n","0.92\n","Fine-tuning...\n","Epoch: 000 Eval Loss: 4.200 Eval Acc: 0.130\n","Epoch: 001 Train Loss: 1.016 Train Acc: 0.731 Eval Loss: 0.866 Eval Acc: 0.756\n","Epoch time: 143.2 Total training time: 154.2\n","\n","Epoch: 002 Train Loss: 0.611 Train Acc: 0.838 Eval Loss: 0.751 Eval Acc: 0.788\n","Epoch time: 143.0 Total training time: 297.2\n","\n","Epoch: 003 Train Loss: 0.488 Train Acc: 0.871 Eval Loss: 0.710 Eval Acc: 0.798\n","Epoch time: 142.8 Total training time: 440.0\n","\n","Epoch: 004 Train Loss: 0.423 Train Acc: 0.891 Eval Loss: 0.687 Eval Acc: 0.803\n","Epoch time: 142.6 Total training time: 582.5\n","\n","Epoch: 005 Train Loss: 0.368 Train Acc: 0.907 Eval Loss: 0.672 Eval Acc: 0.808\n","Epoch time: 142.6 Total training time: 725.2\n","\n","Epoch: 006 Train Loss: 0.336 Train Acc: 0.917 Eval Loss: 0.664 Eval Acc: 0.813\n","Epoch time: 142.7 Total training time: 867.9\n","\n","Epoch: 007 Train Loss: 0.308 Train Acc: 0.924 Eval Loss: 0.660 Eval Acc: 0.814\n","Epoch time: 142.7 Total training time: 1010.6\n","\n","Epoch: 008 Train Loss: 0.288 Train Acc: 0.929 Eval Loss: 0.653 Eval Acc: 0.815\n","Epoch time: 142.7 Total training time: 1153.2\n","\n","Epoch: 009 Train Loss: 0.277 Train Acc: 0.932 Eval Loss: 0.650 Eval Acc: 0.819\n","Epoch time: 142.8 Total training time: 1296.1\n","\n","Epoch: 010 Train Loss: 0.262 Train Acc: 0.936 Eval Loss: 0.647 Eval Acc: 0.819\n","Epoch time: 143.5 Total training time: 1439.6\n","\n","Epoch: 011 Train Loss: 0.252 Train Acc: 0.939 Eval Loss: 0.642 Eval Acc: 0.822\n","Epoch time: 142.6 Total training time: 1582.2\n","\n","Epoch: 012 Train Loss: 0.236 Train Acc: 0.943 Eval Loss: 0.642 Eval Acc: 0.820\n","Epoch time: 142.9 Total training time: 1725.1\n","\n","Epoch: 013 Train Loss: 0.236 Train Acc: 0.943 Eval Loss: 0.644 Eval Acc: 0.822\n","Epoch time: 142.9 Total training time: 1867.9\n","\n","Epoch: 014 Train Loss: 0.225 Train Acc: 0.945 Eval Loss: 0.644 Eval Acc: 0.821\n","Epoch time: 142.8 Total training time: 2010.8\n","\n","Epoch: 015 Train Loss: 0.224 Train Acc: 0.945 Eval Loss: 0.643 Eval Acc: 0.822\n","Epoch time: 142.6 Total training time: 2153.3\n","\n","Test Accuracy: 0.822\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.91      0.92      0.92       100\n","           1       0.93      0.92      0.92       100\n","           2       0.72      0.71      0.72       100\n","           3       0.82      0.71      0.76       100\n","           4       0.76      0.70      0.73       100\n","           5       0.81      0.83      0.82       100\n","           6       0.92      0.89      0.90       100\n","           7       0.84      0.79      0.81       100\n","           8       0.94      0.93      0.93       100\n","           9       0.89      0.94      0.91       100\n","          10       0.73      0.67      0.70       100\n","          11       0.52      0.60      0.56       100\n","          12       0.85      0.88      0.87       100\n","          13       0.85      0.77      0.81       100\n","          14       0.90      0.85      0.88       100\n","          15       0.81      0.91      0.85       100\n","          16       0.88      0.88      0.88       100\n","          17       0.93      0.87      0.90       100\n","          18       0.85      0.86      0.86       100\n","          19       0.85      0.74      0.79       100\n","          20       0.87      0.88      0.88       100\n","          21       0.87      0.94      0.90       100\n","          22       0.95      0.83      0.89       100\n","          23       0.88      0.86      0.87       100\n","          24       0.92      0.89      0.90       100\n","          25       0.76      0.75      0.75       100\n","          26       0.78      0.73      0.75       100\n","          27       0.74      0.73      0.74       100\n","          28       0.85      0.85      0.85       100\n","          29       0.82      0.90      0.86       100\n","          30       0.73      0.82      0.77       100\n","          31       0.86      0.84      0.85       100\n","          32       0.83      0.78      0.80       100\n","          33       0.82      0.68      0.74       100\n","          34       0.85      0.85      0.85       100\n","          35       0.70      0.49      0.58       100\n","          36       0.88      0.91      0.89       100\n","          37       0.87      0.83      0.85       100\n","          38       0.84      0.83      0.83       100\n","          39       0.93      0.93      0.93       100\n","          40       0.89      0.82      0.85       100\n","          41       0.97      0.92      0.94       100\n","          42       0.82      0.82      0.82       100\n","          43       0.92      0.93      0.93       100\n","          44       0.74      0.71      0.72       100\n","          45       0.65      0.76      0.70       100\n","          46       0.62      0.78      0.69       100\n","          47       0.68      0.67      0.67       100\n","          48       0.89      0.96      0.92       100\n","          49       0.84      0.91      0.87       100\n","          50       0.80      0.64      0.71       100\n","          51       0.87      0.87      0.87       100\n","          52       0.62      0.67      0.64       100\n","          53       0.88      0.97      0.92       100\n","          54       0.79      0.90      0.84       100\n","          55       0.61      0.62      0.61       100\n","          56       0.91      0.92      0.92       100\n","          57       0.89      0.84      0.87       100\n","          58       0.91      0.94      0.93       100\n","          59       0.73      0.69      0.71       100\n","          60       0.90      0.79      0.84       100\n","          61       0.82      0.83      0.83       100\n","          62       0.76      0.81      0.78       100\n","          63       0.75      0.86      0.80       100\n","          64       0.75      0.70      0.73       100\n","          65       0.73      0.80      0.76       100\n","          66       0.92      0.88      0.90       100\n","          67       0.80      0.74      0.77       100\n","          68       0.89      0.95      0.92       100\n","          69       0.85      0.91      0.88       100\n","          70       0.77      0.79      0.78       100\n","          71       0.80      0.82      0.81       100\n","          72       0.67      0.65      0.66       100\n","          73       0.85      0.73      0.78       100\n","          74       0.66      0.69      0.67       100\n","          75       0.94      0.91      0.92       100\n","          76       0.93      0.93      0.93       100\n","          77       0.81      0.88      0.84       100\n","          78       0.81      0.76      0.78       100\n","          79       0.82      0.91      0.86       100\n","          80       0.76      0.84      0.80       100\n","          81       0.78      0.86      0.82       100\n","          82       0.96      0.92      0.94       100\n","          83       0.86      0.84      0.85       100\n","          84       0.83      0.79      0.81       100\n","          85       0.92      0.85      0.89       100\n","          86       0.94      0.87      0.90       100\n","          87       0.92      0.88      0.90       100\n","          88       0.86      0.85      0.85       100\n","          89       0.90      0.97      0.93       100\n","          90       0.77      0.85      0.81       100\n","          91       0.87      0.89      0.88       100\n","          92       0.80      0.66      0.73       100\n","          93       0.85      0.82      0.84       100\n","          94       0.95      0.98      0.97       100\n","          95       0.84      0.81      0.83       100\n","          96       0.66      0.71      0.68       100\n","          97       0.83      0.90      0.87       100\n","          98       0.63      0.67      0.65       100\n","          99       0.81      0.88      0.85       100\n","\n","    accuracy                           0.82     10000\n","   macro avg       0.82      0.82      0.82     10000\n","weighted avg       0.82      0.82      0.82     10000\n","\n","Global Sparsity:\n","0.92\n","Pruning and Finetuning 6/6\n","Pruning...\n","Test Accuracy: 0.037\n","Global Sparsity:\n","0.95\n","Fine-tuning...\n","Epoch: 000 Eval Loss: 9.813 Eval Acc: 0.037\n","Epoch: 001 Train Loss: 1.497 Train Acc: 0.609 Eval Loss: 1.098 Eval Acc: 0.692\n","Epoch time: 142.7 Total training time: 153.5\n","\n","Epoch: 002 Train Loss: 0.904 Train Acc: 0.753 Eval Loss: 0.909 Eval Acc: 0.738\n","Epoch time: 142.9 Total training time: 296.4\n","\n","Epoch: 003 Train Loss: 0.726 Train Acc: 0.802 Eval Loss: 0.821 Eval Acc: 0.759\n","Epoch time: 142.6 Total training time: 439.0\n","\n","Epoch: 004 Train Loss: 0.618 Train Acc: 0.833 Eval Loss: 0.776 Eval Acc: 0.773\n","Epoch time: 142.4 Total training time: 581.4\n","\n","Epoch: 005 Train Loss: 0.548 Train Acc: 0.853 Eval Loss: 0.751 Eval Acc: 0.784\n","Epoch time: 142.7 Total training time: 724.0\n","\n","Epoch: 006 Train Loss: 0.496 Train Acc: 0.868 Eval Loss: 0.727 Eval Acc: 0.790\n","Epoch time: 142.5 Total training time: 866.6\n","\n","Epoch: 007 Train Loss: 0.454 Train Acc: 0.882 Eval Loss: 0.719 Eval Acc: 0.793\n","Epoch time: 142.5 Total training time: 1009.1\n","\n","Epoch: 008 Train Loss: 0.415 Train Acc: 0.893 Eval Loss: 0.712 Eval Acc: 0.795\n","Epoch time: 142.4 Total training time: 1151.6\n","\n","Epoch: 009 Train Loss: 0.395 Train Acc: 0.901 Eval Loss: 0.703 Eval Acc: 0.798\n","Epoch time: 142.7 Total training time: 1294.2\n","\n","Epoch: 010 Train Loss: 0.372 Train Acc: 0.906 Eval Loss: 0.703 Eval Acc: 0.798\n","Epoch time: 143.6 Total training time: 1437.8\n","\n","Epoch: 011 Train Loss: 0.355 Train Acc: 0.910 Eval Loss: 0.692 Eval Acc: 0.802\n","Epoch time: 142.8 Total training time: 1580.7\n","\n","Epoch: 013 Train Loss: 0.318 Train Acc: 0.921 Eval Loss: 0.691 Eval Acc: 0.804\n","Epoch time: 142.7 Total training time: 1866.1\n","\n","Epoch: 014 Train Loss: 0.308 Train Acc: 0.924 Eval Loss: 0.689 Eval Acc: 0.803\n","Epoch time: 142.7 Total training time: 2008.8\n","\n","Epoch: 015 Train Loss: 0.299 Train Acc: 0.926 Eval Loss: 0.691 Eval Acc: 0.807\n","Epoch time: 142.8 Total training time: 2151.7\n","\n","Test Accuracy: 0.807\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.86      0.95      0.90       100\n","           1       0.92      0.88      0.90       100\n","           2       0.63      0.73      0.68       100\n","           3       0.73      0.69      0.71       100\n","           4       0.61      0.68      0.64       100\n","           5       0.82      0.84      0.83       100\n","           6       0.88      0.82      0.85       100\n","           7       0.88      0.80      0.84       100\n","           8       0.97      0.91      0.94       100\n","           9       0.87      0.93      0.90       100\n","          10       0.78      0.61      0.69       100\n","          11       0.59      0.54      0.56       100\n","          12       0.87      0.87      0.87       100\n","          13       0.80      0.76      0.78       100\n","          14       0.91      0.86      0.88       100\n","          15       0.81      0.88      0.84       100\n","          16       0.83      0.86      0.84       100\n","          17       0.89      0.85      0.87       100\n","          18       0.83      0.80      0.82       100\n","          19       0.84      0.74      0.79       100\n","          20       0.88      0.88      0.88       100\n","          21       0.93      0.91      0.92       100\n","          22       0.85      0.82      0.84       100\n","          23       0.91      0.86      0.88       100\n","          24       0.92      0.89      0.90       100\n","          25       0.79      0.72      0.75       100\n","          26       0.87      0.75      0.81       100\n","          27       0.74      0.75      0.74       100\n","          28       0.88      0.82      0.85       100\n","          29       0.88      0.84      0.86       100\n","          30       0.77      0.79      0.78       100\n","          31       0.84      0.83      0.83       100\n","          32       0.80      0.74      0.77       100\n","          33       0.79      0.70      0.74       100\n","          34       0.81      0.83      0.82       100\n","          35       0.62      0.54      0.58       100\n","          36       0.80      0.86      0.83       100\n","          37       0.81      0.79      0.80       100\n","          38       0.78      0.79      0.79       100\n","          39       0.94      0.90      0.92       100\n","          40       0.90      0.77      0.83       100\n","          41       0.91      0.92      0.92       100\n","          42       0.81      0.79      0.80       100\n","          43       0.86      0.82      0.84       100\n","          44       0.69      0.77      0.73       100\n","          45       0.70      0.74      0.72       100\n","          46       0.67      0.69      0.68       100\n","          47       0.67      0.72      0.69       100\n","          48       0.90      0.96      0.93       100\n","          49       0.88      0.91      0.89       100\n","          50       0.64      0.69      0.67       100\n","          51       0.82      0.85      0.83       100\n","          52       0.64      0.76      0.69       100\n","          53       0.88      0.96      0.92       100\n","          54       0.83      0.86      0.84       100\n","          55       0.58      0.67      0.62       100\n","          56       0.89      0.90      0.90       100\n","          57       0.79      0.85      0.82       100\n","          58       0.90      0.92      0.91       100\n","          59       0.81      0.63      0.71       100\n","          60       0.88      0.80      0.84       100\n","          61       0.75      0.85      0.79       100\n","          62       0.80      0.75      0.77       100\n","          63       0.75      0.76      0.76       100\n","          64       0.76      0.54      0.63       100\n","          65       0.73      0.77      0.75       100\n","          66       0.84      0.86      0.85       100\n","          67       0.76      0.71      0.74       100\n","          68       0.89      0.93      0.91       100\n","          69       0.84      0.92      0.88       100\n","          70       0.79      0.81      0.80       100\n","          71       0.77      0.86      0.82       100\n","          72       0.62      0.58      0.60       100\n","          73       0.70      0.74      0.72       100\n","          74       0.65      0.71      0.68       100\n","          75       0.94      0.90      0.92       100\n","          76       0.91      0.91      0.91       100\n","          77       0.80      0.85      0.83       100\n","          78       0.74      0.84      0.79       100\n","          79       0.81      0.86      0.83       100\n","          80       0.67      0.78      0.72       100\n","          81       0.81      0.79      0.80       100\n","          82       0.95      0.94      0.94       100\n","          83       0.88      0.76      0.82       100\n","          84       0.89      0.84      0.87       100\n","          85       0.88      0.91      0.89       100\n","          86       0.85      0.86      0.86       100\n","          87       0.89      0.90      0.90       100\n","          88       0.85      0.81      0.83       100\n","          89       0.93      0.90      0.91       100\n","          90       0.77      0.92      0.84       100\n","          91       0.87      0.83      0.85       100\n","          92       0.75      0.75      0.75       100\n","          93       0.83      0.77      0.80       100\n","          94       0.91      0.95      0.93       100\n","          95       0.83      0.81      0.82       100\n","          96       0.64      0.66      0.65       100\n","          97       0.85      0.85      0.85       100\n","          98       0.61      0.68      0.64       100\n","          99       0.89      0.81      0.85       100\n","\n","    accuracy                           0.81     10000\n","   macro avg       0.81      0.81      0.81     10000\n","weighted avg       0.81      0.81      0.81     10000\n","\n","Global Sparsity:\n","0.95\n"]},{"data":{"text/plain":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (2): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (2): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (3): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (2): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (3): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (4): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (5): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (2): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Linear(in_features=512, out_features=100, bias=True)\n",")"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["print(\"Pruning...\")\n","iterative_pruning_finetuning(\n","        model=pruned_model,\n","        train_loader=train_loader,\n","        test_loader=test_loader,\n","        device=cuda_device,\n","        learning_rate=learning_rate,\n","        learning_rate_decay=learning_rate_decay,\n","        l1_regularization_strength=l1_regularization_strength,\n","        l2_regularization_strength=l2_regularization_strength,\n","        weight_decay=weight_decay,\n","        conv2d_prune_amount=0.95,\n","        linear_prune_amount=0,\n","        num_iterations=6,           \n","        num_epochs_per_iteration=10,  \n","        model_filename_prefix=model_filename_prefix,\n","        model_dir=model_dir,\n","        grouped_pruning=True)"]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2024-03-30T16:01:02.116917Z","iopub.status.busy":"2024-03-30T16:01:02.116507Z","iopub.status.idle":"2024-03-30T16:01:02.171796Z","shell.execute_reply":"2024-03-30T16:01:02.170718Z","shell.execute_reply.started":"2024-03-30T16:01:02.116884Z"},"trusted":true},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":51,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-05-06T16:07:56.868196Z","iopub.status.busy":"2024-05-06T16:07:56.867402Z","iopub.status.idle":"2024-05-06T16:07:56.877746Z","shell.execute_reply":"2024-05-06T16:07:56.876718Z","shell.execute_reply.started":"2024-05-06T16:07:56.868160Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"data":{"text/plain":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (2): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (2): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (3): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (2): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (3): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (4): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (5): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (2): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Linear(in_features=512, out_features=100, bias=True)\n",")"]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":["pruned_model"]},{"cell_type":"code","execution_count":39,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-05-07T14:55:03.068040Z","iopub.status.busy":"2024-05-07T14:55:03.067359Z","iopub.status.idle":"2024-05-07T14:55:14.022787Z","shell.execute_reply":"2024-05-07T14:55:14.021732Z","shell.execute_reply.started":"2024-05-07T14:55:03.067990Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.86      0.95      0.90       100\n","           1       0.92      0.88      0.90       100\n","           2       0.63      0.73      0.68       100\n","           3       0.73      0.69      0.71       100\n","           4       0.61      0.68      0.64       100\n","           5       0.82      0.84      0.83       100\n","           6       0.88      0.82      0.85       100\n","           7       0.88      0.80      0.84       100\n","           8       0.97      0.91      0.94       100\n","           9       0.87      0.93      0.90       100\n","          10       0.78      0.61      0.69       100\n","          11       0.59      0.54      0.56       100\n","          12       0.87      0.87      0.87       100\n","          13       0.80      0.76      0.78       100\n","          14       0.91      0.86      0.88       100\n","          15       0.81      0.88      0.84       100\n","          16       0.83      0.86      0.84       100\n","          17       0.89      0.85      0.87       100\n","          18       0.83      0.80      0.82       100\n","          19       0.84      0.74      0.79       100\n","          20       0.88      0.88      0.88       100\n","          21       0.93      0.91      0.92       100\n","          22       0.85      0.82      0.84       100\n","          23       0.91      0.86      0.88       100\n","          24       0.92      0.89      0.90       100\n","          25       0.79      0.72      0.75       100\n","          26       0.87      0.75      0.81       100\n","          27       0.74      0.75      0.74       100\n","          28       0.88      0.82      0.85       100\n","          29       0.88      0.84      0.86       100\n","          30       0.77      0.79      0.78       100\n","          31       0.84      0.83      0.83       100\n","          32       0.80      0.74      0.77       100\n","          33       0.79      0.70      0.74       100\n","          34       0.81      0.83      0.82       100\n","          35       0.62      0.54      0.58       100\n","          36       0.80      0.86      0.83       100\n","          37       0.81      0.79      0.80       100\n","          38       0.78      0.79      0.79       100\n","          39       0.94      0.90      0.92       100\n","          40       0.90      0.77      0.83       100\n","          41       0.91      0.92      0.92       100\n","          42       0.81      0.79      0.80       100\n","          43       0.86      0.82      0.84       100\n","          44       0.69      0.77      0.73       100\n","          45       0.70      0.74      0.72       100\n","          46       0.67      0.69      0.68       100\n","          47       0.67      0.72      0.69       100\n","          48       0.90      0.96      0.93       100\n","          49       0.88      0.91      0.89       100\n","          50       0.64      0.69      0.67       100\n","          51       0.82      0.85      0.83       100\n","          52       0.64      0.76      0.69       100\n","          53       0.88      0.96      0.92       100\n","          54       0.83      0.86      0.84       100\n","          55       0.58      0.67      0.62       100\n","          56       0.89      0.90      0.90       100\n","          57       0.79      0.85      0.82       100\n","          58       0.90      0.92      0.91       100\n","          59       0.81      0.63      0.71       100\n","          60       0.88      0.80      0.84       100\n","          61       0.75      0.85      0.79       100\n","          62       0.80      0.75      0.77       100\n","          63       0.75      0.76      0.76       100\n","          64       0.76      0.54      0.63       100\n","          65       0.73      0.77      0.75       100\n","          66       0.84      0.86      0.85       100\n","          67       0.76      0.71      0.74       100\n","          68       0.89      0.93      0.91       100\n","          69       0.84      0.92      0.88       100\n","          70       0.79      0.81      0.80       100\n","          71       0.77      0.86      0.82       100\n","          72       0.62      0.58      0.60       100\n","          73       0.70      0.74      0.72       100\n","          74       0.65      0.71      0.68       100\n","          75       0.94      0.90      0.92       100\n","          76       0.91      0.91      0.91       100\n","          77       0.80      0.85      0.83       100\n","          78       0.74      0.84      0.79       100\n","          79       0.81      0.86      0.83       100\n","          80       0.67      0.78      0.72       100\n","          81       0.81      0.79      0.80       100\n","          82       0.95      0.94      0.94       100\n","          83       0.88      0.76      0.82       100\n","          84       0.89      0.84      0.87       100\n","          85       0.88      0.91      0.89       100\n","          86       0.85      0.86      0.86       100\n","          87       0.89      0.90      0.90       100\n","          88       0.85      0.81      0.83       100\n","          89       0.93      0.90      0.91       100\n","          90       0.77      0.92      0.84       100\n","          91       0.87      0.83      0.85       100\n","          92       0.75      0.75      0.75       100\n","          93       0.83      0.77      0.80       100\n","          94       0.91      0.95      0.93       100\n","          95       0.83      0.81      0.82       100\n","          96       0.64      0.66      0.65       100\n","          97       0.85      0.85      0.85       100\n","          98       0.61      0.68      0.64       100\n","          99       0.89      0.81      0.85       100\n","\n","    accuracy                           0.81     10000\n","   macro avg       0.81      0.81      0.81     10000\n","weighted avg       0.81      0.81      0.81     10000\n","\n"]}],"source":["classification_report = create_classification_report(\n","        model=pruned_model, test_loader=test_loader, device=cuda_device)\n","print(classification_report)"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T14:56:43.519272Z","iopub.status.busy":"2024-05-07T14:56:43.518869Z","iopub.status.idle":"2024-05-07T14:56:43.532048Z","shell.execute_reply":"2024-05-07T14:56:43.530989Z","shell.execute_reply.started":"2024-05-07T14:56:43.519229Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Global Sparsity:\n","0.95\n"]}],"source":["num_zeros, num_elements, sparsity = measure_global_sparsity(pruned_model, conv2d_use_mask=True)\n","\n","#print(\"Test Accuracy: {:.3f}\".format(eval_accuracy))\n","#print(\"Classification Report:\")\n","#print(classification_report)\n","print(\"Global Sparsity:\")\n","print(\"{:.2f}\".format(sparsity))"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T14:59:23.019679Z","iopub.status.busy":"2024-05-07T14:59:23.019290Z","iopub.status.idle":"2024-05-07T14:59:23.472404Z","shell.execute_reply":"2024-05-07T14:59:23.471405Z","shell.execute_reply.started":"2024-05-07T14:59:23.019638Z"},"trusted":true},"outputs":[],"source":["torch.save({\n","            #'epoch': epoch,\n","            'state_dict': pruned_model.state_dict(),\n","            #'optimizer': optimizer.state_dict(),\n","            }, './checkpoint_iter6.pth.tar')"]},{"cell_type":"code","execution_count":55,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T16:11:17.620690Z","iopub.status.busy":"2024-05-06T16:11:17.619941Z","iopub.status.idle":"2024-05-06T16:11:17.949634Z","shell.execute_reply":"2024-05-06T16:11:17.948818Z","shell.execute_reply.started":"2024-05-06T16:11:17.620661Z"},"trusted":true},"outputs":[],"source":["checkpoint = torch.load(\"/kaggle/working/checkpoint_epoch30.pth.tar\")"]},{"cell_type":"code","execution_count":60,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T16:35:01.338792Z","iopub.status.busy":"2024-05-06T16:35:01.338410Z","iopub.status.idle":"2024-05-06T16:35:03.507760Z","shell.execute_reply":"2024-05-06T16:35:03.506791Z","shell.execute_reply.started":"2024-05-06T16:35:01.338766Z"},"trusted":true},"outputs":[],"source":["a = create_model(num_classes=num_classes)\n","parameters_to_prune = []\n","    # Substitute the FC output layer\n","a.fc = torch.nn.Linear(a.fc.in_features, 100)\n","for module_name, module in a.named_modules():\n","    if isinstance(module, torch.nn.Conv2d):\n","        parameters_to_prune.append((module, \"weight\"))\n","prune.global_unstructured(\n","                        parameters_to_prune,\n","                        pruning_method=prune.L1Unstructured,\n","                        amount=0,\n","                        )\n","\n","    # Load a pretrained model.\n","a = load_model(model=a,\n","                    model_filepath=\"/kaggle/working/checkpoint_epoch30.pth.tar\",\n","                    device=cuda_device)"]},{"cell_type":"code","execution_count":61,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-05-06T16:35:17.711379Z","iopub.status.busy":"2024-05-06T16:35:17.710979Z","iopub.status.idle":"2024-05-06T16:35:29.491346Z","shell.execute_reply":"2024-05-06T16:35:29.490096Z","shell.execute_reply.started":"2024-05-06T16:35:17.711348Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.91      0.94      0.93       100\n","           1       0.90      0.88      0.89       100\n","           2       0.67      0.68      0.68       100\n","           3       0.74      0.65      0.69       100\n","           4       0.71      0.73      0.72       100\n","           5       0.81      0.84      0.82       100\n","           6       0.81      0.81      0.81       100\n","           7       0.83      0.82      0.82       100\n","           8       0.94      0.92      0.93       100\n","           9       0.93      0.90      0.91       100\n","          10       0.73      0.59      0.65       100\n","          11       0.53      0.58      0.55       100\n","          12       0.83      0.83      0.83       100\n","          13       0.77      0.79      0.78       100\n","          14       0.91      0.87      0.89       100\n","          15       0.79      0.89      0.84       100\n","          16       0.86      0.83      0.84       100\n","          17       0.87      0.89      0.88       100\n","          18       0.78      0.80      0.79       100\n","          19       0.82      0.76      0.79       100\n","          20       0.92      0.87      0.89       100\n","          21       0.88      0.91      0.89       100\n","          22       0.83      0.82      0.82       100\n","          23       0.88      0.84      0.86       100\n","          24       0.93      0.87      0.90       100\n","          25       0.76      0.78      0.77       100\n","          26       0.74      0.75      0.74       100\n","          27       0.69      0.72      0.71       100\n","          28       0.89      0.85      0.87       100\n","          29       0.81      0.86      0.83       100\n","          30       0.75      0.86      0.80       100\n","          31       0.82      0.84      0.83       100\n","          32       0.80      0.73      0.76       100\n","          33       0.79      0.67      0.72       100\n","          34       0.77      0.86      0.82       100\n","          35       0.64      0.58      0.61       100\n","          36       0.84      0.85      0.85       100\n","          37       0.84      0.81      0.82       100\n","          38       0.74      0.76      0.75       100\n","          39       0.95      0.94      0.94       100\n","          40       0.78      0.79      0.79       100\n","          41       0.96      0.91      0.93       100\n","          42       0.79      0.82      0.80       100\n","          43       0.90      0.89      0.89       100\n","          44       0.70      0.68      0.69       100\n","          45       0.70      0.71      0.70       100\n","          46       0.61      0.65      0.63       100\n","          47       0.62      0.66      0.64       100\n","          48       0.90      0.98      0.94       100\n","          49       0.80      0.92      0.86       100\n","          50       0.68      0.64      0.66       100\n","          51       0.85      0.83      0.84       100\n","          52       0.64      0.67      0.66       100\n","          53       0.88      0.96      0.92       100\n","          54       0.83      0.87      0.85       100\n","          55       0.53      0.57      0.55       100\n","          56       0.85      0.90      0.87       100\n","          57       0.85      0.88      0.86       100\n","          58       0.91      0.96      0.94       100\n","          59       0.70      0.63      0.66       100\n","          60       0.87      0.86      0.86       100\n","          61       0.78      0.83      0.81       100\n","          62       0.75      0.80      0.77       100\n","          63       0.75      0.75      0.75       100\n","          64       0.78      0.69      0.73       100\n","          65       0.71      0.75      0.73       100\n","          66       0.94      0.90      0.92       100\n","          67       0.75      0.68      0.71       100\n","          68       0.91      0.96      0.93       100\n","          69       0.87      0.88      0.88       100\n","          70       0.82      0.79      0.81       100\n","          71       0.80      0.81      0.81       100\n","          72       0.64      0.57      0.60       100\n","          73       0.74      0.73      0.73       100\n","          74       0.67      0.62      0.64       100\n","          75       0.94      0.91      0.92       100\n","          76       0.88      0.92      0.90       100\n","          77       0.81      0.82      0.82       100\n","          78       0.75      0.76      0.76       100\n","          79       0.84      0.87      0.85       100\n","          80       0.71      0.76      0.73       100\n","          81       0.78      0.77      0.77       100\n","          82       0.94      0.95      0.95       100\n","          83       0.86      0.78      0.82       100\n","          84       0.87      0.80      0.83       100\n","          85       0.90      0.88      0.89       100\n","          86       0.90      0.80      0.85       100\n","          87       0.90      0.92      0.91       100\n","          88       0.89      0.86      0.87       100\n","          89       0.86      0.91      0.88       100\n","          90       0.83      0.84      0.84       100\n","          91       0.89      0.86      0.87       100\n","          92       0.74      0.72      0.73       100\n","          93       0.84      0.73      0.78       100\n","          94       0.86      0.95      0.90       100\n","          95       0.83      0.78      0.80       100\n","          96       0.63      0.69      0.66       100\n","          97       0.85      0.88      0.87       100\n","          98       0.63      0.63      0.63       100\n","          99       0.83      0.80      0.82       100\n","\n","    accuracy                           0.80     10000\n","   macro avg       0.80      0.80      0.80     10000\n","weighted avg       0.80      0.80      0.80     10000\n","\n"]}],"source":["classification_report = create_classification_report(\n","        model=a, test_loader=test_loader, device=cuda_device)\n","print(classification_report)"]},{"cell_type":"code","execution_count":62,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T16:35:49.224635Z","iopub.status.busy":"2024-05-06T16:35:49.223574Z","iopub.status.idle":"2024-05-06T16:35:49.235240Z","shell.execute_reply":"2024-05-06T16:35:49.234219Z","shell.execute_reply.started":"2024-05-06T16:35:49.224593Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0.9477184695908522\n"]}],"source":["num_zeros, num_elements, sparsity = measure_global_sparsity(a, conv2d_use_mask=True)\n","print(sparsity)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = train_model(model, )"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"isSourceIdPinned":true,"modelInstanceId":16433,"sourceId":19809,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":true,"modelInstanceId":37215,"sourceId":44307,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
